#!/usr/bin/env python3
"""
bench_ollama_models.py - Benchmark de modÃ¨les Ollama pour RAG HAProxy

Teste plusieurs modÃ¨les avec les mÃªmes questions et compare :
- QualitÃ© des rÃ©ponses (keywords trouvÃ©s, pertinence)
- Vitesse (tokens/seconde, temps total)
- Utilisation mÃ©moire (taille du modÃ¨le)

Usage:
    uv run python bench_ollama_models.py
    uv run python bench_ollama_models.py --models gemma3:latest,qwen3:latest
    uv run python bench_ollama_models.py --all  # Tous les modÃ¨les disponibles
"""
import argparse
import json
import os
import subprocess
import sys
import time
import io
from pathlib import Path
from typing import Optional

# Fix encoding Windows
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

import requests


# â”€â”€ Questions de test â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TEST_QUESTIONS = [
    {
        "id": "healthcheck",
        "question": "Comment configurer un health check HTTP dans HAProxy ?",
        "expected_keywords": ["option httpchk", "check", "GET", "http-check", "server"],
        "min_length": 200,  # Longueur minimale attendue
    },
    {
        "id": "bind",
        "question": "Quelle est la syntaxe de la directive bind ?",
        "expected_keywords": ["bind", "port", "ssl", "crt", "frontend"],
        "min_length": 150,
    },
    {
        "id": "stick_table",
        "question": "Comment limiter les connexions par IP avec stick-table ?",
        "expected_keywords": ["stick-table", "conn_rate", "track-sc", "deny", "ip"],
        "min_length": 200,
    },
]


# â”€â”€ ModÃ¨les Ã  tester â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ModÃ¨les de gÃ©nÃ©ration de texte (pas d'embedding, pas d'OCR, pas de vision)
DEFAULT_MODELS = [
    "gemma3:latest",      # 3.3 GB - Bon Ã©quilibre qualitÃ©/vitesse
    "gemma3n:latest",     # 7.5 GB - Nouveau gemma (plus grand)
    "qwen3:latest",       # 5.2 GB - Excellent en franÃ§ais
    "qwen3-vl:4b",        # 3.3 GB - Version 4B (vision mais marche pour texte)
    "lfm2.5-thinking:1.2b-bf16",  # 2.3 GB - Petit mais rapide
]

# ModÃ¨les Ã  exclure (embedding, OCR, etc.)
EXCLUDED_PATTERNS = [
    "embed",        # bge-m3, mxbai-embed, qwen3-embedding, nomic-embed
    "ocr",          # glm-ocr
    "bf16",         # Versions brutes non optimisÃ©es (sauf lfm2.5)
]


# â”€â”€ Prompt systÃ¨me (identique pour tous) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SYSTEM_PROMPT = """Tu es un expert HAProxy 3.2.

CONSIGNES :
- RÃ©ponds UNIQUEMENT avec le contexte fourni
- Sois factuel et prÃ©cis
- Utilise des exemples de configuration si pertinent
- FranÃ§ais uniquement
- Structure ta rÃ©ponse

<context>
[Source: 5.2. Server and default-server options]
option httpchk - Enable HTTP protocol to check server health

Syntax: option httpchk [<method> <uri> [<version>]]

When this option is set, a server which returns an HTTP code 2xx or 3xx 
is considered alive, otherwise it is considered dead.

Example:
backend web_servers
    option httpchk GET /health HTTP/1.1
    server web1 192.168.1.1:80 check

[Source: 4.2. Alphabetically sorted keywords reference]
bind - Define listening address and port

Syntax: bind [ip_addr]:port [ssl crt /path/to/cert.pem]

Example:
frontend http
    bind *:80
    bind *:443 ssl crt /etc/ssl/cert.pem

[Source: 11.1. stick-table declaration]
stick-table - Declare a stick table for tracking

Syntax: stick-table type ip size <entries> expire <time> store <counters>

Example:
frontend http
    stick-table type ip size 100k expire 30s store conn_rate(10s)
    http-request track-sc0 src
    http-request deny if { sc0_conn_rate gt 100 }
</context>"""


def get_ollama_url() -> str:
    return os.getenv("OLLAMA_URL", "http://localhost:11434")


def list_available_models() -> list[str]:
    """Liste les modÃ¨les disponibles dans Ollama."""
    try:
        response = requests.get(f"{get_ollama_url()}/api/tags", timeout=10)
        response.raise_for_status()
        return [m["name"] for m in response.json().get("models", [])]
    except Exception as e:
        print(f"âŒ Erreur liste modÃ¨les: {e}")
        return []


def unload_model() -> bool:
    """DÃ©charge le modÃ¨le courant pour libÃ©rer le GPU."""
    try:
        # Ollama dÃ©charge automatiquement les modÃ¨les aprÃ¨s inactivitÃ©
        # On peut forcer avec un appel vide
        response = requests.post(
            f"{get_ollama_url()}/api/generate",
            json={"model": "", "prompt": ""},
            timeout=5,
        )
        return True
    except Exception:
        pass
    
    # Alternative: tuer le processus Ollama (radical)
    print("   âš ï¸  Attente 10s pour libÃ©ration GPU...")
    time.sleep(10)
    return True


def load_model(model_name: str) -> bool:
    """Charge le modÃ¨le en mÃ©moire (warm-up)."""
    print(f"   ğŸ“¥ Chargement de {model_name}...")
    try:
        # Petit appel pour charger le modÃ¨le
        response = requests.post(
            f"{get_ollama_url()}/api/generate",
            json={
                "model": model_name,
                "prompt": "Hello",
                "stream": False,
            },
            timeout=120,
        )
        response.raise_for_status()
        print(f"   âœ… {model_name} chargÃ©")
        return True
    except Exception as e:
        print(f"   âŒ Erreur chargement: {e}")
        return False


def benchmark_model(
    model_name: str,
    questions: list[dict],
) -> dict:
    """
    Benchmark d'un modÃ¨le.
    
    Returns:
        dict avec stats: temps, tokens, qualitÃ©, etc.
    """
    print(f"\n{'='*60}")
    print(f"ğŸ” Benchmark: {model_name}")
    print(f"{'='*60}")
    
    # Charger le modÃ¨le
    if not load_model(model_name):
        return {"error": "Failed to load model"}
    
    results = []
    total_tokens = 0
    total_time = 0
    
    for i, test in enumerate(questions, 1):
        question_id = test["id"]
        question = test["question"]
        expected = test["expected_keywords"]
        
        print(f"\n  Question {i}/{len(questions)}: {question_id}")
        
        # Messages
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": question},
        ]
        
        # Benchmark
        start_time = time.time()
        
        try:
            response = requests.post(
                f"{get_ollama_url()}/api/chat",
                json={
                    "model": model_name,
                    "messages": messages,
                    "stream": False,
                    "options": {
                        "temperature": 0.1,
                        "top_p": 0.9,
                        "repeat_penalty": 1.1,
                        "num_predict": 1024,
                    },
                },
                timeout=300,
            )
            response.raise_for_status()
            
            response_data = response.json()
            answer = response_data["message"]["content"]
            
            # Stats
            elapsed = time.time() - start_time
            tokens = response_data.get("eval_count", 0)
            total_tokens += tokens
            total_time += elapsed
            
            # Tokens/seconde
            tokens_per_sec = tokens / elapsed if elapsed > 0 else 0
            
            # QualitÃ©
            answer_lower = answer.lower()
            found_keywords = [
                kw for kw in expected if kw.lower() in answer_lower
            ]
            keyword_score = len(found_keywords) / len(expected)
            
            # Longueur
            length_ok = len(answer) >= test["min_length"]
            
            # Score global
            quality_score = (
                keyword_score * 0.6 +
                (0.2 if length_ok else 0) +
                (0.2 if tokens > 50 else 0)
            )
            
            result = {
                "question_id": question_id,
                "answer": answer[:200] + "..." if len(answer) > 200 else answer,
                "answer_length": len(answer),
                "elapsed": elapsed,
                "tokens": tokens,
                "tokens_per_sec": round(tokens_per_sec, 2),
                "keyword_score": round(keyword_score, 2),
                "found_keywords": found_keywords,
                "quality_score": round(quality_score, 2),
            }
            results.append(result)
            
            # Affichage
            status = "âœ…" if quality_score > 0.7 else "âš ï¸" if quality_score > 0.4 else "âŒ"
            print(f"    {status} QualitÃ©: {quality_score:.2f}")
            print(f"    â±ï¸  Temps: {elapsed:.2f}s | Tokens: {tokens} | {tokens_per_sec:.1f} tok/s")
            print(f"    ğŸ¯ Keywords: {len(found_keywords)}/{len(expected)}")
            
        except Exception as e:
            print(f"    âŒ Erreur: {e}")
            results.append({
                "question_id": question_id,
                "error": str(e),
                "quality_score": 0,
            })
    
    # DÃ©charger le modÃ¨le
    unload_model()
    
    # Stats globales
    avg_quality = sum(r.get("quality_score", 0) for r in results) / len(results)
    avg_time = sum(r.get("elapsed", 0) for r in results if "elapsed" in r) / max(1, len([r for r in results if "elapsed" in r]))
    avg_tokens_per_sec = total_tokens / total_time if total_time > 0 else 0
    
    return {
        "model": model_name,
        "results": results,
        "stats": {
            "avg_quality": round(avg_quality, 2),
            "avg_time": round(avg_time, 2),
            "avg_tokens_per_sec": round(avg_tokens_per_sec, 1),
            "total_tokens": total_tokens,
            "total_time": round(total_time, 2),
        },
    }


def generate_report(benchmarks: list[dict], output_file: str = "bench_report.json"):
    """GÃ©nÃ¨re un rapport JSON et texte."""
    
    # Filtrer les erreurs
    valid_benchmarks = [b for b in benchmarks if "error" not in b]
    
    if not valid_benchmarks:
        print("\nâŒ Aucun benchmark valide")
        return
    
    # Classement par qualitÃ©
    ranking_quality = sorted(
        valid_benchmarks,
        key=lambda x: x["stats"]["avg_quality"],
        reverse=True,
    )
    
    # Classement par vitesse
    ranking_speed = sorted(
        valid_benchmarks,
        key=lambda x: x["stats"]["avg_tokens_per_sec"],
        reverse=True,
    )
    
    # Rapport JSON
    report_data = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "models_tested": len(valid_benchmarks),
        "ranking_quality": [
            {
                "model": b["model"],
                "avg_quality": b["stats"]["avg_quality"],
                "avg_time": b["stats"]["avg_time"],
                "tokens_per_sec": b["stats"]["avg_tokens_per_sec"],
            }
            for b in ranking_quality
        ],
        "ranking_speed": [
            {
                "model": b["model"],
                "tokens_per_sec": b["stats"]["avg_tokens_per_sec"],
                "avg_quality": b["stats"]["avg_quality"],
            }
            for b in ranking_speed
        ],
        "all_results": benchmarks,
    }
    
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(report_data, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“Š Rapport sauvegardÃ©: {output_file}")
    
    # Rapport texte
    print("\n" + "="*70)
    print("ğŸ“ˆ CLASSEMENT PAR QUALITÃ‰")
    print("="*70)
    
    for i, entry in enumerate(ranking_quality, 1):
        medal = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else "  "
        print(f"{medal} {i}. {entry['model']}")
        print(f"     QualitÃ©: {entry['avg_quality']:.2f}/1.0 | "
              f"Temps: {entry['avg_time']:.2f}s | "
              f"Vitesse: {entry['tokens_per_sec']:.1f} tok/s")
    
    print("\n" + "="*70)
    print("âš¡ CLASSEMENT PAR VITESSE")
    print("="*70)
    
    for i, entry in enumerate(ranking_speed, 1):
        medal = "ğŸš€" if i == 1 else "  "
        print(f"{medal} {i}. {entry['model']}")
        print(f"     Vitesse: {entry['tokens_per_sec']:.1f} tok/s | "
              f"QualitÃ©: {entry['avg_quality']:.2f}/1.0")
    
    # Recommandations
    print("\n" + "="*70)
    print("ğŸ’¡ RECOMMANDATIONS")
    print("="*70)
    
    best_quality = ranking_quality[0]
    best_speed = ranking_speed[0]
    
    print(f"âœ… Meilleure qualitÃ©: {best_quality['model']}")
    print(f"âš¡ Meilleure vitesse: {best_speed['model']}")
    
    # Compromis qualitÃ©/vitesse
    best_compromise = max(
        valid_benchmarks,
        key=lambda x: x["stats"]["avg_quality"] * x["stats"]["avg_tokens_per_sec"],
    )
    print(f"ğŸ¯ Meilleur compromis: {best_compromise['model']}")
    
    if best_quality["avg_quality"] < 0.6:
        print("\nâš ï¸  Tous les modÃ¨les ont des scores < 0.6")
        print("   â†’ Essayez un modÃ¨le plus grand ou fine-tunez")


def main():
    parser = argparse.ArgumentParser(description="Benchmark de modÃ¨les Ollama")
    parser.add_argument(
        "--models",
        type=str,
        default=",".join(DEFAULT_MODELS),
        help="ModÃ¨les Ã  tester (comma-separated)",
    )
    parser.add_argument(
        "--all",
        action="store_true",
        help="Tester tous les modÃ¨les disponibles",
    )
    parser.add_argument(
        "--output",
        type=str,
        default="bench_report.json",
        help="Fichier de rapport",
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Afficher les rÃ©ponses complÃ¨tes",
    )
    
    args = parser.parse_args()
    
    # VÃ©rifier Ollama
    print("ğŸ” VÃ©rification d'Ollama...")
    try:
        response = requests.get(f"{get_ollama_url()}/api/tags", timeout=10)
        response.raise_for_status()
        available_models = [m["name"] for m in response.json().get("models", [])]
        print(f"âœ… {len(available_models)} modÃ¨les disponibles")
    except Exception as e:
        print(f"âŒ Ollama inaccessible: {e}")
        print("   Lancez: ollama serve")
        sys.exit(1)
    
    # SÃ©lection des modÃ¨les
    if args.all:
        models_to_test = available_models
    else:
        models_to_test = [m.strip() for m in args.models.split(",")]
        # Filtrer les modÃ¨les indisponibles
        models_to_test = [m for m in models_to_test if m in available_models]
    
    if not models_to_test:
        print("âŒ Aucun modÃ¨le Ã  tester")
        sys.exit(1)
    
    print(f"\nğŸ“‹ ModÃ¨les Ã  tester: {models_to_test}")
    print(f"ğŸ“ Questions: {len(TEST_QUESTIONS)}")
    
    # Benchmarks
    benchmarks = []
    for model in models_to_test:
        try:
            result = benchmark_model(model, TEST_QUESTIONS)
            benchmarks.append(result)
        except Exception as e:
            print(f"\nâŒ Erreur pour {model}: {e}")
            benchmarks.append({
                "model": model,
                "error": str(e),
                "results": [],
                "stats": {"avg_quality": 0, "avg_time": 0, "avg_tokens_per_sec": 0},
            })
    
    # Rapport
    generate_report(benchmarks, args.output)
    
    print("\nâœ… Benchmark terminÃ© !")


if __name__ == "__main__":
    main()
