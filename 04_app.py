"""
04_app.py - Interface Gradio : chatbot HAProxy avec RAG hybride
Lance avec : python 06_app.py

Compatible Gradio 6.x - Format messages: {"role": "...", "content": "..."}
"""
import sys
import io
import threading
import logging
import traceback
from pathlib import Path

# Configurer l'encodage UTF-8 pour la console Windows
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('gradio_app.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

try:
    import gradio as gr
except ImportError:
    print("âŒ gradio non installÃ© : uv add gradio")
    sys.exit(1)

# Importer nos modules
sys.path.insert(0, str(Path(__file__).parent))
logger.info("Importation des modules personnalisÃ©s...")
try:
    from retriever import retrieve_context_string, _load_indexes
    logger.info("âœ… Module retriever importÃ© avec succÃ¨s")
except ImportError as e:
    logger.error("âŒ Erreur d'importation du module retriever: %s", e)
    raise

try:
    from llm import (
        generate_response,
        list_ollama_models,
        FALLBACK_RESPONSE,
        DEFAULT_MODEL,
    )
    logger.info("âœ… Module llm importÃ© avec succÃ¨s")
    logger.info("ModÃ¨le par dÃ©faut: %s", DEFAULT_MODEL)
except ImportError as e:
    logger.error("âŒ Erreur d'importation du module llm: %s", e)
    raise


# â”€â”€ CSS personnalisÃ© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CUSTOM_CSS = """
.gradio-container { max-width: 1100px !important; margin: auto; }
.app-title { text-align: center; padding: 10px 0 5px 0; }
.app-title h1 { font-size: 1.8em; color: #2c3e50; margin: 0; }
.app-title p  { color: #7f8c8d; margin: 4px 0 0 0; font-size: 0.9em; }
.sources-box {
    background: #f8f9fa;
    border: 1px solid #dee2e6;
    border-radius: 8px;
    padding: 10px 14px;
    font-size: 0.85em;
    margin-top: 8px;
}
.sources-box .source-item { margin: 6px 0; }
.btn-index { background: #27ae60 !important; color: white !important; }
.btn-clear  { background: #e74c3c !important; color: white !important; }
.low-confidence {
    background: #fff3cd;
    border-left: 4px solid #ffc107;
    padding: 8px 12px;
    border-radius: 4px;
    font-size: 0.85em;
    margin-top: 8px;
}
"""


# â”€â”€ State global â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_indexes_loaded = False
_index_lock     = threading.Lock()


def ensure_indexes():
    """Charge les index une seule fois de maniÃ¨re thread-safe."""
    global _indexes_loaded
    with _index_lock:
        if not _indexes_loaded:
            try:
                logger.info("Tentative de chargement des index...")
                _load_indexes()
                _indexes_loaded = True
                logger.info("âœ… Index chargÃ©s avec succÃ¨s")
                return True, "âœ… Index chargÃ©s"
            except FileNotFoundError as e:
                logger.error("âŒ Index introuvables: %s", e)
                return False, f"âŒ Index introuvables: {e}"
            except Exception as e:
                logger.error("âŒ Erreur chargement index: %s", e)
                return False, f"âŒ Erreur: {e}"
    return True, "âœ… Index dÃ©jÃ  chargÃ©s"


def format_sources_markdown(sources: list[dict]) -> str:
    """Formate les sources en Markdown."""
    if not sources:
        return ""

    lines = ["\n\n---\n\n**ğŸ“š Sources :**\n"]
    for i, src in enumerate(sources):
        icon = "ğŸ“" if src.get("has_code") else "ğŸ“„"
        title = src.get("title", "Section inconnue")
        url = src.get("url", "#")
        score = src.get("score", 0)
        lines.append(f"{icon} **[{i+1}]** [{title}]({url}) (score: {score:.2f})")

    return "\n".join(lines)


def history_to_llm_format(history: list[dict]) -> list[tuple[str, str]]:
    """Convertit l'historique Gradio (format messages) vers format LLM (tuples)."""
    llm_history = []
    user_msg = None

    for msg in history:
        role = msg.get("role", "")
        raw_content = msg.get("content", "")
        
        # Extraire le texte (Gradio 6.x peut avoir content=liste)
        content = extract_message_text(raw_content)

        if role == "user":
            user_msg = content
        elif role == "assistant" and user_msg is not None:
            llm_history.append((user_msg, content))
            user_msg = None

    return llm_history[-3:]  # Garder les 3 derniers tours


def extract_message_text(message) -> str:
    """
    Extrait le texte d'un message Gradio 6.x.
    Gradio 6.x peut passer soit une chaÃ®ne, soit un dict {'text': '...', 'type': 'text'}
    """
    if isinstance(message, str):
        return message
    elif isinstance(message, dict):
        return message.get("text", "") or message.get("content", "")
    elif isinstance(message, list):
        # Peut Ãªtre une liste de contenus
        parts = []
        for item in message:
            if isinstance(item, dict):
                parts.append(item.get("text", "") or item.get("content", ""))
            elif isinstance(item, str):
                parts.append(item)
        return " ".join(parts)
    return str(message)


def submit_message(
    message,
    history: list[dict],
    model_name: str,
    top_k: int,
    show_sources: bool,
):
    """
    Ajoute le message utilisateur Ã  l'historique.
    Retourne: history mis Ã  jour
    """
    # Extraire le texte du message (Gradio 6.x format)
    message_text = extract_message_text(message)
    logger.info("submit_message() - message='%s...'", message_text[:30] if message_text else "")

    if not message_text.strip():
        return history

    # VÃ©rifier les index
    ok, status = ensure_indexes()
    if not ok:
        history.append({"role": "assistant", "content": status})
        return history

    history.append({"role": "user", "content": message_text})
    return history


def respond(
    history: list[dict],
    model_name: str,
    top_k: int,
    show_sources: bool,
):
    """
    GÃ©nÃ¨re la rÃ©ponse de l'assistant avec streaming.
    Format Gradio 6.x : history = [{'role': 'user'|'assistant', 'content': '...'}]
    """
    if not history or history[-1].get("role") != "user":
        return history

    # Extraire le texte du dernier message
    raw_message = history[-1]["content"]
    message = extract_message_text(raw_message)
    logger.info("respond() - message='%s...', model='%s', top_k=%d",
                message[:30] if message else "", model_name, top_k)

    # â”€â”€ Retrieval â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    try:
        logger.info("Retrieval pour: '%s'", message[:80])
        context_str, sources, low_confidence = retrieve_context_string(
            message, top_k=top_k
        )
        logger.info("Retrieval terminÃ© - %d sources, low_confidence=%s",
                   len(sources), low_confidence)
    except Exception as e:
        logger.error("âŒ Erreur retrieval: %s", e)
        history.append({"role": "assistant", "content": f"âŒ Erreur retrieval : {e}"})
        yield history
        return

    # â”€â”€ Cas confiance faible â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if low_confidence or not context_str:
        response = FALLBACK_RESPONSE
        history.append({"role": "assistant", "content": response})
        yield history
        return

    # â”€â”€ Historique pour LLM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    llm_history = history_to_llm_format(history[:-1])  # Exclure le dernier message user

    # â”€â”€ Streaming â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    history.append({"role": "assistant", "content": "â³ Recherche en cours..."})
    yield history

    # Streamer la rÃ©ponse
    history[-1]["content"] = ""
    logger.info("GÃ©nÃ©ration avec modÃ¨le: %s", model_name)

    try:
        for token in generate_response(
            question=message,
            context=context_str,
            model=model_name,
            history=llm_history,
            temperature=0.1,
        ):
            history[-1]["content"] += token
            yield history

        logger.info("GÃ©nÃ©ration terminÃ©e - %d caractÃ¨res", len(history[-1]["content"]))

    except Exception as e:
        logger.error("âŒ Erreur gÃ©nÃ©ration: %s", e)
        history[-1]["content"] = f"âŒ Erreur gÃ©nÃ©ration : {e}"
        yield history
        return

    # Ajouter les sources
    if show_sources and sources:
        history[-1]["content"] += format_sources_markdown(sources)

    # Warning confiance faible
    if low_confidence:
        history[-1]["content"] += '\n\n<div class="low-confidence">âš ï¸ Confiance faible â€” vÃ©rifiez la documentation officielle</div>'

    yield history


def reindex_fn():
    """Lance le pipeline de rÃ©indexation complet."""
    import subprocess

    logger.info("DÃ©but de la rÃ©indexation")

    for script in ["01_scrape.py", "02_ingest_v2.py", "03_build_index_v2.py"]:
        path = Path(__file__).parent / script
        if not path.exists():
            yield f"âŒ Script introuvable : {script}"
            return

        yield f"ğŸ”„ {script}..."
        logger.info("ExÃ©cution de %s...", script)

        try:
            result = subprocess.run(
                [sys.executable, str(path)],
                capture_output=True,
                text=True,
                cwd=str(Path(__file__).parent),
                timeout=300,
            )
        except subprocess.TimeoutExpired:
            yield f"âŒ Timeout pour {script}"
            return
        except Exception as e:
            yield f"âŒ Erreur pour {script}: {e}"
            return

        if result.returncode != 0:
            yield f"âŒ Erreur dans {script}:\n{result.stderr[:500]}"
            return

        yield f"âœ… {script} terminÃ©"

    global _indexes_loaded
    _indexes_loaded = False

    yield "âœ… RÃ©indexation complÃ¨te ! Rechargez la page pour utiliser les nouveaux index."


# â”€â”€ Construction de l'UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_ui():
    """Construit l'interface Gradio."""
    logger.info("Construction de l'UI...")

    # ModÃ¨les disponibles
    try:
        available_models = list_ollama_models()
    except Exception:
        available_models = []

    if not available_models:
        available_models = [DEFAULT_MODEL, "llama3.1:8b", "qwen2.5:7b"]

    default_model = DEFAULT_MODEL if DEFAULT_MODEL in available_models else (
        available_models[0] if available_models else DEFAULT_MODEL
    )

    logger.info("ModÃ¨les disponibles: %s", available_models)
    logger.info("ModÃ¨le par dÃ©faut: %s", default_model)

    with gr.Blocks(title="HAProxy Docs Chatbot", css=CUSTOM_CSS) as app:

        # Header
        gr.HTML("""
            <div class="app-title">
                <h1>ğŸ”§ HAProxy 3.2 Documentation Assistant</h1>
                <p>Posez vos questions sur la configuration, le management et les directives HAProxy</p>
            </div>
        """)

        with gr.Row():
            # Sidebar
            with gr.Column(scale=1, min_width=240):
                gr.Markdown("### âš™ï¸ ParamÃ¨tres")

                model_dd = gr.Dropdown(
                    choices=available_models,
                    value=default_model,
                    label="ModÃ¨le Ollama",
                )
                top_k = gr.Slider(
                    minimum=1, maximum=10, value=5, step=1,
                    label="Chunks de contexte (top-k)"
                )
                sources_chk = gr.Checkbox(
                    value=True, label="Afficher les sources"
                )

                gr.Markdown("---\n### ğŸ”„ RÃ©indexation")
                ridx_btn = gr.Button("ğŸ”„ RÃ©indexer", variant="secondary")
                ridx_status = gr.Markdown("")

                gr.Markdown("---\n### ğŸ’¡ Exemples")
                examples = [
                    "Comment configurer un health check HTTP ?",
                    "Syntaxe de la directive bind ?",
                    "Limiter les connexions par IP ?",
                    "Comment utiliser les ACLs ?",
                    "Options de timeout disponibles ?",
                    "Configurer SSL/TLS sur un frontend ?",
                ]
                example_btns = [
                    gr.Button(q, size="sm", variant="secondary") for q in examples
                ]

            # Chat zone
            with gr.Column(scale=3):
                chatbot = gr.Chatbot(
                    label="Conversation",
                    height=560,
                    render_markdown=True,
                    avatar_images=(None, "ğŸ”§"),
                )

                with gr.Row():
                    msg_box = gr.Textbox(
                        placeholder="Ex: Comment configurer un health check TCP ?",
                        show_label=False,
                        scale=5,
                        lines=2,
                        container=False,
                    )
                    send_btn = gr.Button("ğŸš€ Envoyer", variant="primary", scale=1)

                clear_btn = gr.Button("ğŸ—‘ï¸ Effacer l'historique", size="sm")

        # Bindings - Gradio 6.x avec format messages
        chatbot_events = (
            msg_box.submit(
                fn=submit_message,
                inputs=[msg_box, chatbot, model_dd, top_k, sources_chk],
                outputs=[chatbot],
            )
            .then(
                fn=respond,
                inputs=[chatbot, model_dd, top_k, sources_chk],
                outputs=[chatbot],
            )
        )

        send_btn.click(
            fn=submit_message,
            inputs=[msg_box, chatbot, model_dd, top_k, sources_chk],
            outputs=[chatbot],
        ).then(
            fn=respond,
            inputs=[chatbot, model_dd, top_k, sources_chk],
            outputs=[chatbot],
        )

        msg_box.submit(
            fn=submit_message,
            inputs=[msg_box, chatbot, model_dd, top_k, sources_chk],
            outputs=[chatbot],
        ).then(
            fn=respond,
            inputs=[chatbot, model_dd, top_k, sources_chk],
            outputs=[chatbot],
        )

        clear_btn.click(
            fn=lambda: [],
            outputs=[chatbot],
        )

        ridx_btn.click(fn=reindex_fn, outputs=ridx_status)

        # Exemples
        for btn, q in zip(example_btns, examples):
            btn.click(fn=lambda x=q: x, outputs=msg_box)

        gr.Markdown(
            "---\nğŸ’¬ Documentation HAProxy 3.2 | "
            "ğŸ  [docs.haproxy.org](https://docs.haproxy.org/3.2/)"
        )

    return app


# â”€â”€ Entry point â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="HAProxy RAG Chatbot")
    parser.add_argument("--host", default="0.0.0.0", help="Adresse d'Ã©coute")
    parser.add_argument("--port", default=7860, type=int, help="Port")
    parser.add_argument("--share", action="store_true", help="Partager via Gradio")
    args = parser.parse_args()

    print("\n" + "=" * 55)
    print("  ğŸ”§ HAProxy 3.2 Documentation Assistant")
    print("=" * 55)
    print(f"  URL       : http://{args.host}:{args.port}")
    print(f"  Ollama    : http://localhost:11434")
    print(f"  ModÃ¨le    : {DEFAULT_MODEL}")
    print(f"  Gradio    : {gr.__version__}")
    print("=" * 55 + "\n")

    # PrÃ©chargement des index
    print("ğŸ“‚ PrÃ©chargement des index...")
    ok, status = ensure_indexes()
    print(f"   {status}")

    try:
        models = list_ollama_models()
        print(f"   ModÃ¨les  : {models}")
    except Exception:
        print(f"   âŒ Erreur modÃ¨les")
        models = []

    print()

    try:
        app = build_ui()
        print("ğŸš€ Lancement de l'application Gradio...\n")
        app.launch(
            server_name=args.host,
            server_port=args.port,
            share=args.share,
            show_error=True,
        )
    except Exception as e:
        logger.critical("âŒ Erreur critique: %s", e)
        print(f"\nâŒ ERREUR: {e}")
        print("Consultez 'gradio_app.log' pour plus de dÃ©tails.")
