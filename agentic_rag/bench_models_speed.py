#!/usr/bin/env python3
"""
Benchmark rapide pour comparer la vitesse des mod√®les Ollama.
"""

import time
import sys
import requests
from pathlib import Path

if sys.platform == 'win32':
    sys.stdout = open(sys.stdout.fileno(), mode='w', encoding='utf-8', buffering=1, newline=None)

def unload_ollama_model():
    """Force Ollama √† d√©charger le mod√®le actuel de la m√©moire."""
    try:
        # Appel √† l'API Ollama pour g√©n√©rer un token vide avec keep_alive=0
        # √áa force le d√©chargement du mod√®le en m√©moire
        response = requests.post(
            'http://localhost:11434/api/generate',
            json={
                'model': 'gemma3:12b',  # N'importe quel mod√®le
                'prompt': '',
                'stream': False,
                'keep_alive': 0  # D√©charge imm√©diatement
            },
            timeout=5
        )
        print(f"   üßπ Ollama memory cleared (status: {response.status_code})")
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Could not clear Ollama memory: {e}")

# Questions de test (5 questions repr√©sentatives)
TEST_QUESTIONS = [
    "Comment configurer un health check HTTP dans HAProxy ?",
    "Comment limiter les connexions par IP avec stick-table ?",
    "Comment cr√©er une ACL bas√©e sur le chemin URL ?",
    "Comment configurer SSL/TLS sur un frontend ?",
    "Comment ajouter un serveur dans un backend HAProxy ?",
]

MODELS = [
    "gemma3:12b",
    "granite4:7b-a1b-h",
    "qwen3:latest"
]

print("=" * 80)
print("üöÄ BENCHMARK VITESSE MOD√àLES OLLAMA")
print("=" * 80)

for i, model in enumerate(MODELS):
    print(f"\nüìä Test du mod√®le: {model}")
    print("-" * 60)
    
    # Nettoyer la m√©moire avant chaque test (sauf le premier)
    if i > 0:
        unload_ollama_model()
    
    try:
        from langchain_ollama import ChatOllama
        
        llm = ChatOllama(
            model=model,
            temperature=0.1,
            num_ctx=2048,
        )
        
        # Test simple (sans tools, juste g√©n√©ration)
        total_time = 0
        results = []
        
        for j, question in enumerate(TEST_QUESTIONS, 1):
            t0 = time.time()
            response = llm.invoke(f"R√©ponds en 2-3 phrases: {question}")
            elapsed = time.time() - t0
            total_time += elapsed
            results.append(elapsed)
            print(f"   Q{j}: {elapsed:.1f}s | {len(response.content)} chars")
        
        avg_time = total_time / len(TEST_QUESTIONS)
        print(f"\n   ‚è±Ô∏è  Moyenne: {avg_time:.1f}s/question")
        print(f"   ‚è±Ô∏è  Total: {total_time:.1f}s pour 5 questions")
        
    except Exception as e:
        print(f"   ‚ùå Erreur: {e}")
        print(f"   Le mod√®le '{model}' est-il disponible ? (ollama pull {model})")

# Nettoyer la m√©moire √† la fin
print("\nüßπ Nettoyage final...")
unload_ollama_model()

print("\n" + "=" * 80)
print("üí° Conseil: Le mod√®le le plus rapide est id√©al pour le RAG")
print("=" * 80)
