#!/usr/bin/env python3
"""
05_bench_targeted.py - Benchmark V3 ciblÃ© sur des questions spÃ©cifiques

Usage:
    # Questions spÃ©cifiques
    uv run python 05_bench_targeted.py --questions full_backend_name,full_server_weight
    
    # Par niveau (quick, standard, full)
    uv run python 05_bench_targeted.py --level full
    
    # ModÃ¨le personnalisÃ©
    uv run python 05_bench_targeted.py --level quick --model qwen3:latest
"""
import argparse
import json
import sys
import io
import time
import requests

# Fix encoding Windows
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# Importer les questions depuis bench_questions.py (module de donnÃ©es)
import importlib.util
spec = importlib.util.spec_from_file_location("bench_questions", "bench_questions.py")
bench_questions = importlib.util.module_from_spec(spec)
spec.loader.exec_module(bench_questions)
QUESTIONS = bench_questions.QUESTIONS
get_questions_by_level = bench_questions.get_questions_by_level


# â”€â”€ Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_MODEL = "qwen3:latest"
OLLAMA_URL = "http://localhost:11434"

SYSTEM_PROMPT = """Tu es un expert HAProxy 3.2.

RÃˆGLES ABSOLUES :
1. RÃ©ponds UNIQUEMENT Ã  partir du contexte entre <context> et </context>
2. Si l'information n'est PAS dans le contexte â†’ dis "Cette information n'est pas dans la documentation fournie"
3. JAMAIS d'invention, JAMAIS de suppositions
4. Cite TOUJOURS la source entre parenthÃ¨ses : (Source: nom_de_la_section)
5. Pour les exemples de configuration, utilise des blocs de code avec la syntaxe haproxy

STRUCTURE OBLIGATOIRE :
1. RÃ©ponse directe (1-2 phrases)
2. DÃ©tails techniques
3. Exemple de configuration (si pertinent)
4. Sources utilisÃ©es"""


PROMPT_TEMPLATE = """<context>
{context}
</context>

Question : {question}"""


def get_ollama_url() -> str:
    return OLLAMA_URL


def load_retriever_v3():
    """Charge le retriever V3."""
    try:
        from retriever_v3 import retrieve_context_string
        return retrieve_context_string
    except ImportError as e:
        print(f"âŒ Erreur import retriever V3: {e}")
        return None


def generate_response(query: str, context: str, model: str) -> tuple[str, float, int]:
    """GÃ©nÃ¨re une rÃ©ponse avec Ollama."""
    prompt = PROMPT_TEMPLATE.format(context=context, question=query)
    
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": prompt},
    ]
    
    start_time = time.time()
    
    try:
        response = requests.post(
            f"{get_ollama_url()}/api/chat",
            json={
                "model": model,
                "messages": messages,
                "stream": False,
                "options": {
                    "temperature": 0.1,
                    "top_p": 0.9,
                    "repeat_penalty": 1.1,
                    "num_predict": 1024,
                },
            },
            timeout=300,
        )
        response.raise_for_status()
        
        elapsed = time.time() - start_time
        response_data = response.json()
        answer = response_data["message"]["content"]
        tokens = response_data.get("eval_count", 0)
        
        return answer, elapsed, tokens
        
    except Exception as e:
        elapsed = time.time() - start_time
        return f"Erreur: {e}", elapsed, 0


def evaluate_answer(answer: str, expected_keywords: list[str], min_length: int) -> dict:
    """Ã‰value la qualitÃ© d'une rÃ©ponse."""
    answer_lower = answer.lower()
    found_keywords = [kw for kw in expected_keywords if kw.lower() in answer_lower]
    keyword_score = len(found_keywords) / len(expected_keywords)
    length_ok = len(answer) >= min_length
    
    quality_score = (
        keyword_score * 0.6 +
        (0.2 if length_ok else 0) +
        (0.2 if len(answer) > 50 else 0)
    )
    
    return {
        "keyword_score": round(keyword_score, 2),
        "length_ok": length_ok,
        "quality_score": round(quality_score, 2),
        "found_keywords": found_keywords,
        "answer_length": len(answer),
    }


def benchmark_targeted(
    retrieve_func,
    model: str,
    questions: list[dict],
    verbose: bool = False,
) -> dict:
    """Benchmark ciblÃ© sur des questions spÃ©cifiques."""
    print(f"\n{'='*70}")
    print(f"ğŸ¯ Benchmark V3 CIBLÃ‰")
    print(f"{'='*70}")
    print(f"   ModÃ¨le LLM: {model}")
    print(f"   Questions: {len(questions)}")
    
    results = []
    total_retrieval_time = 0
    total_generation_time = 0
    
    for i, test in enumerate(questions, 1):
        question_id = test["id"]
        question = test["question"]
        expected = test["expected_keywords"]
        
        progress_bar = "â–ˆ" * i + "â–‘" * (len(questions) - i)
        print(f"\n[{progress_bar}] Question {i}/{len(questions)}: {question_id}")
        if verbose:
            print(f"   Question: {question}")
        
        # Retrieval
        retrieval_start = time.time()
        try:
            context = retrieve_func(question)
        except Exception as e:
            print(f"   âŒ Erreur retrieval: {e}")
            results.append({
                "question_id": question_id,
                "error": str(e),
                "quality_score": 0,
            })
            continue
        
        retrieval_time = time.time() - retrieval_start
        total_retrieval_time += retrieval_time
        
        # GÃ©nÃ©ration
        answer, gen_time, tokens = generate_response(question, context, model)
        total_generation_time += gen_time
        
        # Ã‰valuation
        eval_result = evaluate_answer(answer, expected, test["min_length"])
        
        # Affichage
        status = "âœ…" if eval_result["quality_score"] > 0.7 else "âš ï¸" if eval_result["quality_score"] > 0.4 else "âŒ"
        print(f"   {status} QualitÃ©: {eval_result['quality_score']:.2f}/1.0")
        print(f"   â±ï¸  Retrieval: {retrieval_time:.2f}s | GÃ©nÃ©ration: {gen_time:.1f}s")
        print(f"   ğŸ¯ Keywords: {len(eval_result['found_keywords'])}/{len(expected)}")
        if verbose and eval_result['found_keywords']:
            print(f"   ğŸ“ TrouvÃ©s: {', '.join(eval_result['found_keywords'])}")
        
        results.append({
            "question_id": question_id,
            "answer": answer[:200] + "..." if len(answer) > 200 else answer,
            "retrieval_time": round(retrieval_time, 2),
            "generation_time": round(gen_time, 2),
            **eval_result,
        })
    
    # Stats
    valid_results = [r for r in results if "error" not in r]
    avg_quality = sum(r["quality_score"] for r in valid_results) / len(valid_results) if valid_results else 0
    questions_resolues = sum(1 for r in valid_results if r["quality_score"] > 0.7)
    taux_reussite = (questions_resolues / len(valid_results) * 100) if valid_results else 0
    
    return {
        "index": "V3",
        "model": model,
        "questions_tested": len(questions),
        "results": results,
        "stats": {
            "avg_quality": round(avg_quality, 3),
            "questions_resolues": questions_resolues,
            "taux_reussite": round(taux_reussite, 1),
        },
    }


def generate_report(benchmark: dict, output_file: str = "bench_v3_targeted_report.json"):
    """GÃ©nÃ¨re un rapport."""
    stats = benchmark["stats"]
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(benchmark, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“Š Rapport sauvegardÃ©: {output_file}\n")
    
    print("="*70)
    print("ğŸ“ˆ RÃ‰SULTATS BENCHMARK V3 CIBLÃ‰")
    print("="*70)
    
    print(f"\nğŸ¯ ModÃ¨le LLM: {benchmark['model']}")
    print(f"ğŸ“ Questions: {benchmark['questions_tested']}")
    
    print("\n" + "-"*70)
    print(f"{'MÃ©trique':<30} | {'Valeur':<15}")
    print("-"*70)
    print(f"{'QualitÃ© moyenne':<30} | {stats['avg_quality']:<15.3f}/1.0")
    print(f"{'Questions rÃ©solues':<30} | {stats['questions_resolues']}/{benchmark['questions_tested']:<14}")
    print(f"{'Taux de rÃ©ussite':<30} | {stats['taux_reussite']:<15.1f}%")
    print("-"*70)
    
    # InterprÃ©tation
    print("\n" + "="*70)
    print("ğŸ’¡ INTERPRÃ‰TATION")
    print("="*70)
    
    if stats['avg_quality'] >= 0.90:
        print("âœ… EXCELLENT - QualitÃ© >= 0.90/1.0")
    elif stats['avg_quality'] >= 0.80:
        print("âœ… TRÃˆS BON - QualitÃ© >= 0.80/1.0")
    elif stats['avg_quality'] >= 0.70:
        print("âš ï¸  BON - QualitÃ© >= 0.70/1.0")
    else:
        print("âŒ MOYEN - QualitÃ© < 0.70/1.0")
    
    if stats['taux_reussite'] >= 80:
        print(f"âœ… {stats['taux_reussite']:.1f}% des questions rÃ©solues (objectif >= 80%)")
    else:
        print(f"âš ï¸  {stats['taux_reussite']:.1f}% des questions rÃ©solues (objectif >= 80%)")
    
    # Questions ratÃ©es
    failed = [r for r in benchmark["results"] if r.get("quality_score", 0) <= 0.7]
    if failed:
        print(f"\nâš ï¸  Questions Ã  amÃ©liorer ({len(failed)}/{benchmark['questions_tested']}):")
        for r in failed:
            print(f"   - {r['question_id']}: {r.get('quality_score', 0):.2f}/1.0")
    
    # Comparaison
    print("\n" + "="*70)
    print("ğŸ“Š COMPARAISON AVEC AVANT")
    print("="*70)
    print("Avant (V2) : backend=0.00, weight=0.20")
    print(f"AprÃ¨s (V3) : {stats['avg_quality']:.3f} ({stats['taux_reussite']:.1f}% rÃ©solues)")
    
    if stats['avg_quality'] > 0.70:
        print("\nâœ… AMÃ‰LIORATION CONFIRMÃ‰E !")
    else:
        print("\nâš ï¸  AmÃ©lioration insuffisante")


def main():
    parser = argparse.ArgumentParser(description="Benchmark V3 ciblÃ©")
    parser.add_argument(
        "--model",
        type=str,
        default=DEFAULT_MODEL,
        help=f"ModÃ¨le LLM (dÃ©faut: {DEFAULT_MODEL})",
    )
    parser.add_argument(
        "--questions",
        type=str,
        default="",
        help="Questions Ã  tester (comma-separated)",
    )
    parser.add_argument(
        "--level",
        type=str,
        choices=["quick", "standard", "full"],
        default="",
        help="Niveau de benchmark (quick=7, standard=20, full=100 questions)",
    )
    parser.add_argument(
        "--output",
        type=str,
        default="bench_v3_targeted_report.json",
        help="Fichier de rapport",
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Afficher plus de dÃ©tails",
    )

    args = parser.parse_args()

    # VÃ©rifier Ollama
    print("ğŸ” VÃ©rification d'Ollama...", flush=True)
    try:
        response = requests.get(f"{get_ollama_url()}/api/tags", timeout=10)
        response.raise_for_status()
        available_models = [m["name"] for m in response.json().get("models", [])]
        print(f"âœ… {len(available_models)} modÃ¨les disponibles\n", flush=True)

        if args.model not in available_models:
            print(f"âš ï¸  ModÃ¨le '{args.model}' non disponible", flush=True)
            sys.exit(1)

    except Exception as e:
        print(f"âŒ Ollama inaccessible: {e}", flush=True)
        sys.exit(1)

    # Charger retriever
    print("ğŸ“¥ Chargement du retriever V3...", flush=True)
    retrieve_v3 = load_retriever_v3()

    if not retrieve_v3:
        print("âŒ Impossible de charger le retriever V3", flush=True)
        sys.exit(1)

    print("âœ… Retriever V3 chargÃ©\n", flush=True)

    # SÃ©lectionner les questions
    questions_to_test = []
    
    if args.level:
        # Utiliser un niveau prÃ©dÃ©fini
        questions_to_test = get_questions_by_level(args.level)
        print(f"ğŸ“‹ Niveau: {args.level.upper()} ({len(questions_to_test)} questions)", flush=True)
    elif args.questions:
        # Questions spÃ©cifiques
        question_ids = [q.strip() for q in args.questions.split(",")]
        questions_to_test = [q for q in QUESTIONS if q["id"] in question_ids]
        print(f"ğŸ“‹ Questions ciblÃ©es: {len(questions_to_test)}", flush=True)
        for q in questions_to_test:
            print(f"   - {q['id']} ({q['category']})", flush=True)
    else:
        print("âŒ SpÃ©cifiez --level (quick/standard/full) ou --questions", flush=True)
        print("\nExemples:")
        print("  uv run python 05_bench_targeted.py --level quick")
        print("  uv run python 05_bench_targeted.py --level full")
        print("  uv run python 05_bench_targeted.py --questions full_backend_name,full_server_weight")
        sys.exit(1)

    print(f"ğŸ“‹ ModÃ¨le LLM: {args.model}", flush=True)
    print(f"â±ï¸  Temps estimÃ©: ~{len(questions_to_test) * 25:.0f}s\n", flush=True)
    
    # Benchmark
    try:
        result = benchmark_targeted(retrieve_v3, args.model, questions_to_test, args.verbose)
    except Exception as e:
        print(f"\nâŒ Erreur benchmark: {e}", flush=True)
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    # Rapport
    generate_report(result, args.output)
    
    print("\nâœ… Benchmark V3 ciblÃ© terminÃ© !\n", flush=True)


if __name__ == "__main__":
    main()
