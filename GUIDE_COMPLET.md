# ğŸ“˜ Guide Complet : Pipeline RAG HAProxy

**Version :** V3 (qwen3-embedding:8b, MTEB 70.58)  
**Date :** 2026-02-25  
**Statut :** âœ… PrÃªt pour production

---

## ğŸ“‹ Table des MatiÃ¨res

1. [Vue d'ensemble](#vue-densemble)
2. [PrÃ©requis](#prÃ©requis)
3. [Ã‰tape 1 : Scraper la documentation](#etape-1-scraper-la-documentation)
4. [Ã‰tape 1.5 : Enrichir les metadata IA](#etape-15-enrichir-les-metadata-ia)
5. [Ã‰tape 2 : Parser et chunker](#etape-2-parser-et-chunker)
6. [Ã‰tape 3 : Construire l'index](#etape-3-construire-lindex)
7. [Ã‰tape 4 : Lancer le chatbot](#etape-4-lancer-le-chatbot)
8. [Ã‰tape 5 : Benchmarker](#etape-5-benchmarker)
9. [Architecture technique](#architecture-technique)
10. [DÃ©pannage](#depannage)

---

## ğŸ¯ Vue d'ensemble

Ce projet implÃ©mente un **chatbot RAG (Retrieval-Augmented Generation)** pour la documentation HAProxy 3.2.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PIPELINE RAG COMPLET                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  docs.haproxy.org                                               â”‚
â”‚       â”‚                                                         â”‚
â”‚       â–¼                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  Scrapping  â†’  data/sections.jsonl            â”‚
â”‚  â”‚ 01_scrape.pyâ”‚                                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                â”‚
â”‚       â”‚                                                         â”‚
â”‚       â–¼                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  Enrichissement  â†’  data/sections_enrichies.jsonl â”‚
â”‚  â”‚01b_enrich_meta  â”‚  IA (gemma3)     â”‚  (keywords, synonyms,    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚   category, summary)     â”‚
â”‚       â”‚                                                         â”‚
â”‚       â–¼                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  Chunking   â†’  data/chunks_v2.jsonl           â”‚
â”‚  â”‚02_chunking  â”‚              + propagation metadata IA        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                â”‚
â”‚       â”‚                                                         â”‚
â”‚       â–¼                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  Indexing   â†’  index_v3/                      â”‚
â”‚  â”‚03_indexing  â”‚              - chroma/ (embeddings)           â”‚
â”‚  â”‚.py          â”‚              - bm25.pkl (lexical)             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              - chunks.pkl (metadata)          â”‚
â”‚       â”‚                                                         â”‚
â”‚       â–¼                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  RAG Chat   â†’  Gradio UI                      â”‚
â”‚  â”‚ 04_chatbot  â”‚              - Retrieval hybride              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              - LLM (qwen3:latest)             â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ PrÃ©requis

### **SystÃ¨me**
- **OS :** Windows 10/11, Linux, macOS
- **Python :** 3.11+
- **RAM :** 16GB minimum (32GB recommandÃ©)
- **Stockage :** 10GB libre

### **Logiciels**

#### 1. **uv** (package manager)
```bash
# Windows/Mac/Linux
curl -LsSf https://astral.sh/uv/install.sh | sh

# VÃ©rifier
uv --version
```

#### 2. **Ollama** (LLM local)
```bash
# TÃ©lÃ©charger : https://ollama.com

# Installer les modÃ¨les
ollama pull qwen3-embedding:8b    # Embedding (MTEB #1)
ollama pull qwen3:latest          # LLM (gÃ©nÃ©ration)
ollama pull bge-m3                # Embedding alternatif (V2)
ollama pull gemma3:latest         # LLM alternatif

# VÃ©rifier
ollama list
ollama serve
```

---

## ğŸ“¥ Installation

```bash
# 1. Cloner le repo
git clone <repo-url>
cd haproxy-dataset-generator

# 2. CrÃ©er l'environnement virtuel
uv sync

# 3. VÃ©rifier les modÃ¨les Ollama
ollama list
# Doit afficher : qwen3-embedding:8b, qwen3:latest
```

---

## Ã‰tape 1 : Scraper la documentation

**Objectif :** TÃ©lÃ©charger la documentation HAProxy en Markdown.

### **Execution**

```bash
uv run python 01_scrape.py
```

### **Ce que Ã§a fait :**
- TÃ©lÃ©charge les pages de https://docs.haproxy.org/3.2/
- Convertit le HTML en Markdown
- Nettoie le contenu (headers, footers, pubs)
- Sauvegarde dans `data/sections.jsonl`

### **Sortie attendue :**
```
2026-02-25 10:00:00 - INFO - Scraping HAProxy 3.2 documentation...
2026-02-25 10:05:00 - INFO - âœ… 150 sections scrapÃ©es
2026-02-25 10:05:00 - INFO - ğŸ“ data/sections.jsonl (2.5 MB)
```

### **Contenu de `data/sections.jsonl` :**
```json
{"id": "section_5.2", "title": "5.2. Server and default-server options", "content": "...", "url": "https://docs.haproxy.org/3.2/configuration.html#5.2"}
{"id": "section_4.2", "title": "4.2. Alphabetically sorted keywords reference", "content": "...", "url": "https://docs.haproxy.org/3.2/configuration.html#4.2"}
...
```

### **DurÃ©e :** ~5-10 minutes

---

## Ã‰tape 1.5 : Enrichir les metadata IA

**Objectif :** GÃ©nÃ©rer des mÃ©tadonnÃ©es sÃ©mantiques pour chaque section via IA.

### **Execution**

```bash
uv run python 01b_enrich_metadata.py
```

### **Ce que Ã§a fait :**
- Lit `data/sections.jsonl`
- Pour chaque section, appelle gemma3:latest pour gÃ©nÃ©rer :
  - `keywords` (5-10 mots-clÃ©s techniques prÃ©sents dans le texte)
  - `synonyms` (3-5 termes associÃ©s, variantes)
  - `summary` (1 phrase rÃ©sumÃ©)
  - `category` (backend/frontend/acl/ssl/timeout/healthcheck/stick-table/logs/stats/general/loadbalancing)
- Sauvegarde dans `data/sections_enriched.jsonl`

### **Sortie attendue :**
```
[INFO] 150 sections chargees depuis data/sections.jsonl
[INFO] Modele IA: gemma3:latest
[INFO] Temps estime: ~5 min (150 sections x 2s)

[1/150] 5.2. Server and default-server options... OK 8 keywords | backend
[2/150] 4.2. Alphabetically sorted keywords reference... OK 7 keywords | general
...

[SUCCESS] ENRICHISSEMENT TERMINE
[INFO] Sections enrichies: 150
[INFO] Keywords totaux: 1050 (7.0/section)
[INFO] Categories:
   backend: 45 (30%)
   general: 30 (20%)
   acl: 25 (17%)
   ...
```

### **Contenu de `data/sections_enriched.jsonl` :**
```json
{
  "id": "section_5.2",
  "title": "5.2. Server and default-server options",
  "content": "...",
  "url": "https://docs.haproxy.org/3.2/configuration.html#5.2",
  "metadata": {
    "keywords": ["server", "backend", "weight", "check", "option httpchk", "inter", "fall", "rise"],
    "synonyms": ["dÃ©sactiver", "disabled", "down", "inactive"],
    "summary": "Configuration des serveurs backend et options de health check HTTP.",
    "category": "backend"
  }
}
...
```

### **DurÃ©e :** ~5-10 minutes (gemma3:latest)

---

## Ã‰tape 2 : Parser et chunker

**Objectif :** DÃ©couper la documentation en chunks intelligents avec propagation des metadata IA.

### **Execution**

```bash
uv run python 02_chunking.py
```

### **Ce que Ã§a fait :**
- Lit `data/sections_enriched.jsonl` (avec metadata IA)
- DÃ©coupe en chunks de ~300-800 caractÃ¨res (chunking sÃ©mantique)
- Propage les metadata IA aux chunks (ia_keywords, ia_synonyms, ia_category, ia_summary)
- Ajoute des metadata HAProxy (tags, keywords, section hierarchy)
- Sauvegarde dans `data/chunks_v2.jsonl`

### **Sortie attendue :**
```
[INFO] 150 sections enrichies chargees depuis data/sections_enriched.jsonl
   Apres fusion : 180 sections
[INFO] Resultat re-chunking V2:
   Chunks totaux     : 3645
   Avec code         : 2121 (58%)
   Taille moy.       : 669 chars
   Min / Max         : 150 / 1200
   Tags HAProxy      : 10935 (3.0/chunk)
   Avec metadata IA  : 3645 (100%)
   IA keywords tot.  : 25515 (7.0/chunk)

[SUCCESS] 3645 chunks sauvegardes dans data/chunks_v2.jsonl
```

### **Contenu de `data/chunks_v2.jsonl` :**
```json
{
  "id": "chunk_0",
  "title": "5.2. Server and default-server options",
  "content": "option httpchk - Enable HTTP protocol to check server health...",
  "embed_text": "5.2. Server and default-server options\n\noption httpchk...",
  "url": "https://docs.haproxy.org/3.2/configuration.html#5.2",
  "source": "configuration",
  "parent_section": "5",
  "current_section": "5.2",
  "tags": ["healthcheck", "httpchk", "server", "option"],
  "keywords": ["healthcheck", "httpchk", "server", "option", "backend", "weight"],
  "has_code": true,
  "char_len": 669,
  "ia_keywords": ["server", "backend", "weight", "check", "option httpchk", "inter", "fall", "rise"],
  "ia_synonyms": ["dÃ©sactiver", "disabled", "down", "inactive"],
  "ia_category": "backend",
  "ia_summary": "Configuration des serveurs backend et options de health check HTTP."
}
...
```

### **DurÃ©e :** ~5 minutes

---

## Ã‰tape 3 : Construire l'index

**Objectif :** CrÃ©er les index vectoriels et lexicaux avec metadata IA.

### **Execution**

```bash
uv run python 03_indexing.py
```

### **Ce que Ã§a fait :**
- Charge `data/chunks_v2.jsonl` (3645 chunks avec metadata IA)
- GÃ©nÃ¨re les embeddings avec `qwen3-embedding:8b` (4096 dims)
- CrÃ©e l'index ChromaDB (vectoriel) avec metadata :
  - `ia_keywords` : Keywords IA de gemma3:latest
  - `ia_synonyms` : Synonymes IA
  - `ia_category` : CatÃ©gorie IA
  - `ia_summary` : RÃ©sumÃ© IA
  - `keywords` : Keywords combines (HAProxy + IA)
  - `tags` : Tags HAProxy
- CrÃ©e l'index BM25 (lexical)
- Sauvegarde les metadata

### **Sortie attendue :**
```
2026-02-25 10:20:00 - INFO - ============================================================
2026-02-25 10:20:00 - INFO - ğŸ” Indexation V3 - HAProxy RAG
2026-02-25 10:20:00 - INFO - ============================================================
2026-02-25 10:20:00 - INFO - ğŸ“‚ 3645 chunks charges depuis data/chunks_v2.jsonl
2026-02-25 10:20:00 - INFO - âœ… Ollama OK - Modele: qwen3-embedding:8b
2026-02-25 10:20:00 - INFO -
ğŸ“¦ Indexation de 3645 chunks...
2026-02-25 10:20:00 - INFO -    Progression: 500/3645 (13%) | ETA: ~104 min
2026-02-25 10:20:00 - INFO -    Progression: 1000/3645 (27%) | ETA: ~88 min
...
2026-02-25 12:31:22 - INFO -    Progression: 3645/3645 (100%) | ETA: ~0 min
2026-02-25 12:31:22 - INFO - âœ… Index BM25 sauvegarde: index_v3/bm25.pkl
2026-02-25 12:31:22 - INFO - âœ… Chunks sauvegardes: index_v3/chunks.pkl
2026-02-25 12:31:22 - INFO -
âœ… Index V3 termine !
   Collection ChromaDB: haproxy_docs_v3
   Nombre de chunks: 3645
   Dimensions embedding: 4096
```
```

### **Fichiers gÃ©nÃ©rÃ©s :**
```
index_v3/
â”œâ”€â”€ chroma/           # Index vectoriel ChromaDB
â”‚   â”œâ”€â”€ chroma.sqlite3
â”‚   â””â”€â”€ ...
â”œâ”€â”€ bm25.pkl          # Index lexical BM25
â””â”€â”€ chunks.pkl        # Metadata des chunks
```

### **DurÃ©e :** ~2 heures (135 min)

---

## Ã‰tape 4 : Lancer le chatbot

**Objectif :** Interface Gradio pour poser des questions.

### **Execution**

```bash
uv run python 04_chatbot.py
```

### **Ce que Ã§a fait :**
- Charge les index V3 (ChromaDB + BM25)
- Lance un serveur Gradio
- Interface web pour poser des questions
- Retrieval hybride (vectoriel + lexical + rerank)
- GÃ©nÃ©ration de rÃ©ponse avec `qwen3:latest`
- Utilise les metadata IA pour le keyword boosting

### **Sortie attendue :**
```
2026-02-25 13:00:00 - INFO - Importation des modules V3 (qwen3-embedding:8b)...
2026-02-25 13:00:00 - INFO - âœ… Module retriever_v3 importÃ© avec succÃ¨s
2026-02-25 13:00:00 - INFO - âœ… Module llm importÃ© avec succÃ¨s
2026-02-25 13:00:00 - INFO - ModÃ¨le par dÃ©faut: qwen3:latest
2026-02-25 13:00:00 - INFO - Tentative de chargement des index V3...
2026-02-25 13:00:01 - INFO - âœ… Index V3 chargÃ©s avec succÃ¨s (qwen3-embedding:8b)
2026-02-25 13:00:01 - INFO - Starting Gradio server...
* Running on local URL: http://localhost:7860
```

### **Interface :**
- **URL :** http://localhost:7860
- **FonctionnalitÃ©s :**
  - Chat avec historique
  - Affichage des sources
  - Exemples de questions
  - Toggle "Montrer les sources"

### **Exemples de questions :**
- "Comment configurer un health check HTTP ?"
- "Syntaxe de la directive bind ?"
- "Comment limiter les connexions par IP ?"
- "Comment utiliser les ACLs ?"
- "Options de timeout disponibles ?"

---

## Ã‰tape 5 : Benchmarker

**Objectif :** Mesurer la performance du RAG.

### **Niveaux de benchmark**

#### **Quick (7 questions, ~3 min)**
```bash
uv run python 05_bench_targeted.py --level quick
```

#### **Standard (20 questions, ~8 min)**
```bash
uv run python 05_bench_targeted.py --level standard
```

#### **Full (92 questions, ~45 min)**
```bash
uv run python 05_bench_targeted.py --level full
```

### **Benchmark ciblÃ© (questions spÃ©cifiques)**
```bash
# Tester des questions spÃ©cifiques
uv run python 05_bench_targeted.py --questions full_backend_name,full_acl_status
```

### **Comparaison V2 vs V3**
```bash
# Script supprimÃ© - utiliser 05_bench_targeted.py directement
uv run python 05_bench_targeted.py --level full
```

### **Benchmark des modÃ¨les LLM**
```bash
uv run python 06_bench_ollama.py --all
```

### **RÃ©sultats attendus (Full 100 questions) :**
```
======================================================================
ğŸ“ˆ RÃ‰SULTATS BENCHMARK V3
======================================================================

ğŸ¯ ModÃ¨le LLM: qwen3:latest
ğŸ“ Questions: 100
   Index: index_v3/ (qwen3-embedding:8b, MTEB 70.58)

----------------------------------------------------------------------
MÃ©trique                       | Valeur
----------------------------------------------------------------------
QualitÃ© moyenne                | 0.846          /1.0
Taux de rÃ©ussite (>0.7)        | 82.0           %
Questions rÃ©solues             : 82/100
Temps de retrieval moy.        : 6.84           s
Temps de gÃ©nÃ©ration moy.       : 15.58          s
Temps total                    : 2241.90        s
Tokens moy.                    : 504.1
----------------------------------------------------------------------

======================================================================
ğŸ’¡ INTERPRÃ‰TATION
======================================================================
âœ… TRÃˆS BON - QualitÃ© >= 0.80/1.0
âœ… 82.0% des questions rÃ©solues (objectif >= 80%)
```

---

## ğŸ—ï¸ Architecture technique

### **Pipeline de retrieval**

```
Question utilisateur
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Query Expansion    â”‚  Ajoute synonymes techniques
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ChromaDB Search    â”‚  Top-50 vectoriel (cosine)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BM25 Search        â”‚  Top-50 lexical
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RRF Fusion         â”‚  Combine les 2 scores
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FlashRank Rerank   â”‚  Top-10 avec cross-encoder
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Metadata Filtering â”‚  Filtre par section (optionnel)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Keyword Boosting   â”‚  Booste les chunks avec keywords
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM Generation     â”‚  qwen3:latest avec contexte
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
RÃ©ponse finale avec sources
```

### **Configurations clÃ©s**

#### **`retriever_v3.py`**
```python
TOP_K_RETRIEVAL      = 50  # Candidats par mÃ©thode
TOP_K_RRF            = 30  # AprÃ¨s fusion RRF
TOP_K_RERANK         = 10  # AprÃ¨s reranking
RRF_K                = 60  # ParamÃ¨tre RRF
CONFIDENCE_THRESHOLD = 0.0  # Seuil de confiance
```

#### **Metadata Filtering**
```python
SECTION_HINTS = {
    "stick-table": ["11.1", "11.2", "7.3", "11.3"],
    "acl": ["7.1", "7.2", "7.3", "7.4", "7.5", "8.1", "8.2"],
    "backend": ["5.1", "5.2", "5.3", "4.1", "4.3", "3.1"],
    "ssl": ["4.2", "5.1", "5.3", "3.1", "4.1"],
    ...
}
```

---

## ğŸ”§ DÃ©pannage

### **ProblÃ¨me : Ollama inaccessible**
```bash
# VÃ©rifier qu'Ollama tourne
ollama serve

# VÃ©rifier les modÃ¨les
ollama list

# RÃ©installer un modÃ¨le
ollama rm qwen3-embedding:8b
ollama pull qwen3-embedding:8b
```

### **ProblÃ¨me : Index manquants**
```bash
# Reconstruire l'index
uv run python 03_indexing.py

# Ou supprimer et reconstruire
rm -rf index_v3/
uv run python 03_indexing.py
```

### **ProblÃ¨me : ChromaDB error**
```bash
# Supprimer le cache ChromaDB
rm -rf index_v3/chroma/chroma.sqlite3

# Reconstruire
uv run python 03_build_index_v3.py
```

### **ProblÃ¨me : Gradio ne dÃ©marre pas**
```bash
# VÃ©rifier les dÃ©pendances
uv sync

# RÃ©installer Gradio
uv add gradio
```

### **ProblÃ¨me : QualitÃ© faible**
```bash
# VÃ©rifier le retrieval sur des questions spÃ©cifiques
uv run python 05_bench_targeted.py --questions full_backend_name,full_acl_status --verbose

# Ajuster SECTION_HINTS dans retriever_v3.py
# Reconstruire l'index si nÃ©cessaire
```

---

## ğŸ“Š Performance attendue

| MÃ©trique | Valeur | Objectif |
|----------|--------|----------|
| QualitÃ© moyenne | 0.846/1.0 | 0.80+ âœ… |
| Questions rÃ©solues | 82% | 80%+ âœ… |
| Temps/requÃªte | 22.4s | <25s âœ… |
| Chunks indexÃ©s | 3645 | - |
| Taille index | ~500 MB | - |

---

## ğŸ“ Structure des fichiers

```
haproxy-dataset-generator/
â”œâ”€â”€ 00_rebuild_all.py         # â­ Script unique - Reconstruit tout
â”œâ”€â”€ 01_scrape.py              # Scrapping â†’ sections.jsonl
â”œâ”€â”€ 01b_enrich_metadata.py    # Enrichissement IA â†’ sections_enriched.jsonl
â”œâ”€â”€ 02_chunking.py            # Chunking + propagation metadata â†’ chunks_v2.jsonl
â”œâ”€â”€ 03_indexing.py            # Indexing â†’ index_v3/
â”œâ”€â”€ 04_chatbot.py             # Chatbot Gradio
â”œâ”€â”€ retriever_v3.py           # Retrieval hybride V3
â”œâ”€â”€ llm.py                    # GÃ©nÃ©ration LLM
â”œâ”€â”€ bench_questions.py        # 92 questions de benchmark
â”œâ”€â”€ 05_bench_targeted.py      # Benchmark (quick/standard/full)
â”œâ”€â”€ 06_bench_ollama.py        # Benchmark modÃ¨les LLM
â”œâ”€â”€ V3_PERFORMANCE_TRACKING.md# Historique des perfs
â”œâ”€â”€ data/                     # DonnÃ©es brutes
â”‚   â”œâ”€â”€ sections.jsonl
â”‚   â”œâ”€â”€ sections_enriched.jsonl
â”‚   â””â”€â”€ chunks_v2.jsonl
â””â”€â”€ index_v3/                 # Index construits
    â”œâ”€â”€ chroma/
    â”œâ”€â”€ bm25.pkl
    â””â”€â”€ chunks.pkl
```

---

## ğŸš€ Commandes rapides

```bash
# Installation
uv sync
ollama pull qwen3-embedding:8b
ollama pull qwen3:latest
ollama pull gemma3:latest         # Pour l'enrichissement metadata

# Reconstruction complÃ¨te (~3h10)
uv run python 00_rebuild_all.py

# Ou Ã©tapes individuelles :

# Scrapping (~5-10 min)
uv run python 01_scrape.py

# Enrichissement metadata IA (~5-10 min)
uv run python 01b_enrich_metadata.py

# Chunking (~5-10 min)
uv run python 02_chunking.py

# Indexing (~2h)
uv run python 03_indexing.py

# Chatbot
uv run python 04_chatbot.py

# Benchmark Quick (3 min)
uv run python 05_bench_targeted.py --level quick

# Benchmark Full (45 min)
uv run python 05_bench_targeted.py --level full
```

---

## ğŸ“š Ressources

- [HAProxy 3.2 Docs](https://docs.haproxy.org/3.2/)
- [Ollama](https://ollama.com)
- [ChromaDB](https://docs.trychroma.com/)
- [FlashRank](https://github.com/PrithivirajDamodaran/FlashRank)
- [Qwen3 Embedding](https://ollama.com/library/qwen3-embedding:8b)

---

**DerniÃ¨re mise Ã  jour :** 2026-02-25  
**Version :** V3 (qwen3-embedding:8b, MTEB 70.58)  
**Statut :** âœ… PrÃªt pour production
