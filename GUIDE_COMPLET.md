# ğŸ“˜ Guide Complet : Pipeline RAG HAProxy

**Version :** V3 (qwen3-embedding:8b, MTEB 70.58)  
**Date :** 2026-02-25  
**Statut :** âœ… PrÃªt pour production

---

## ğŸ“‹ Table des MatiÃ¨res

1. [Vue d'ensemble](#vue-densemble)
2. [PrÃ©requis](#prÃ©requis)
3. [Ã‰tape 1 : Scraper la documentation](#etape-1-scraper-la-documentation)
4. [Ã‰tape 2 : Parser et chunker](#etape-2-parser-et-chunker)
5. [Ã‰tape 3 : Construire l'index](#etape-3-construire-lindex)
6. [Ã‰tape 4 : Lancer le chatbot](#etape-4-lancer-le-chatbot)
7. [Ã‰tape 5 : Benchmarker](#etape-5-benchmarker)
8. [Architecture technique](#architecture-technique)
9. [DÃ©pannage](#depannage)

---

## ğŸ¯ Vue d'ensemble

Ce projet implÃ©mente un **chatbot RAG (Retrieval-Augmented Generation)** pour la documentation HAProxy 3.2.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PIPELINE RAG COMPLET                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  docs.haproxy.org                                               â”‚
â”‚       â”‚                                                         â”‚
â”‚       â–¼                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  Scrapping  â†’  data/sections.jsonl            â”‚
â”‚  â”‚ 01_scrape.pyâ”‚                                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                â”‚
â”‚       â”‚                                                         â”‚
â”‚       â–¼                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  Chunking   â†’  data/chunks_v3.jsonl           â”‚
â”‚  â”‚02_ingest_v2 â”‚                                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                â”‚
â”‚       â”‚                                                         â”‚
â”‚       â–¼                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  Indexing   â†’  index_v3/                      â”‚
â”‚  â”‚03_build_    â”‚              - chroma/ (embeddings)           â”‚
â”‚  â”‚ index_v3.py â”‚              - bm25.pkl (lexical)             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              - chunks.pkl (metadata)          â”‚
â”‚       â”‚                                                         â”‚
â”‚       â–¼                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  RAG Chat   â†’  Gradio UI                      â”‚
â”‚  â”‚ 04_app_v3.pyâ”‚              - Retrieval hybride              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              - LLM (qwen3:latest)             â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ PrÃ©requis

### **SystÃ¨me**
- **OS :** Windows 10/11, Linux, macOS
- **Python :** 3.11+
- **RAM :** 16GB minimum (32GB recommandÃ©)
- **Stockage :** 10GB libre

### **Logiciels**

#### 1. **uv** (package manager)
```bash
# Windows/Mac/Linux
curl -LsSf https://astral.sh/uv/install.sh | sh

# VÃ©rifier
uv --version
```

#### 2. **Ollama** (LLM local)
```bash
# TÃ©lÃ©charger : https://ollama.com

# Installer les modÃ¨les
ollama pull qwen3-embedding:8b    # Embedding (MTEB #1)
ollama pull qwen3:latest          # LLM (gÃ©nÃ©ration)
ollama pull bge-m3                # Embedding alternatif (V2)
ollama pull gemma3:latest         # LLM alternatif

# VÃ©rifier
ollama list
ollama serve
```

---

## ğŸ“¥ Installation

```bash
# 1. Cloner le repo
git clone <repo-url>
cd haproxy-dataset-generator

# 2. CrÃ©er l'environnement virtuel
uv sync

# 3. VÃ©rifier les modÃ¨les Ollama
ollama list
# Doit afficher : qwen3-embedding:8b, qwen3:latest
```

---

## Ã‰tape 1 : Scraper la documentation

**Objectif :** TÃ©lÃ©charger la documentation HAProxy en Markdown.

### **Execution**

```bash
uv run python 01_scrape.py
```

### **Ce que Ã§a fait :**
- TÃ©lÃ©charge les pages de https://docs.haproxy.org/3.2/
- Convertit le HTML en Markdown
- Nettoie le contenu (headers, footers, pubs)
- Sauvegarde dans `data/sections.jsonl`

### **Sortie attendue :**
```
2026-02-25 10:00:00 - INFO - Scraping HAProxy 3.2 documentation...
2026-02-25 10:05:00 - INFO - âœ… 150 sections scrapÃ©es
2026-02-25 10:05:00 - INFO - ğŸ“ data/sections.jsonl (2.5 MB)
```

### **Contenu de `data/sections.jsonl` :**
```json
{"id": "section_5.2", "title": "5.2. Server and default-server options", "content": "...", "url": "https://docs.haproxy.org/3.2/configuration.html#5.2"}
{"id": "section_4.2", "title": "4.2. Alphabetically sorted keywords reference", "content": "...", "url": "https://docs.haproxy.org/3.2/configuration.html#4.2"}
...
```

### **DurÃ©e :** ~5-10 minutes

---

## Ã‰tape 2 : Parser et chunker

**Objectif :** DÃ©couper la documentation en chunks intelligents.

### **Execution**

```bash
uv run python 02_ingest_v2.py
```

### **Ce que Ã§a fait :**
- Lit `data/sections.jsonl`
- DÃ©coupe en chunks de ~500-800 caractÃ¨res
- Respecte les limites de sections (---)
- Ajoute des metadata (title, section, tags, has_code)
- Sauvegarde dans `data/chunks_v3.jsonl`

### **Sortie attendue :**
```
2026-02-25 10:10:00 - INFO - Chunking intelligent...
2026-02-25 10:15:00 - INFO - âœ… 3645 chunks crÃ©Ã©s
2026-02-25 10:15:00 - INFO - ğŸ“ data/chunks_v3.jsonl (5.2 MB)
2026-02-25 10:15:00 - INFO - ğŸ“Š Stats:
  - Taille moy: 669 chars
  - Tags moy: 3/chunk
  - Avec code: 2121 (58%)
```

### **Contenu de `data/chunks_v3.jsonl` :**
```json
{
  "chunk_id": "chunk_0",
  "title": "5.2. Server and default-server options",
  "section": "5.2",
  "content": "option httpchk - Enable HTTP protocol to check server health\n\nSyntax: option httpchk [<method> <uri> [<version>]]\n\nWhen this option is set...",
  "tags": ["healthcheck", "httpchk", "server", "option"],
  "has_code": true,
  "url": "https://docs.haproxy.org/3.2/configuration.html#5.2"
}
...
```

### **DurÃ©e :** ~5 minutes

---

## Ã‰tape 3 : Construire l'index

**Objectif :** CrÃ©er les index vectoriels et lexicaux.

### **Execution**

```bash
uv run python 03_build_index_v3.py
```

### **Ce que Ã§a fait :**
- Charge `data/chunks_v3.jsonl` (3645 chunks)
- GÃ©nÃ¨re les embeddings avec `qwen3-embedding:8b` (4096 dims)
- CrÃ©e l'index ChromaDB (vectoriel)
- CrÃ©e l'index BM25 (lexical)
- Sauvegarde les metadata

### **Sortie attendue :**
```
2026-02-25 10:20:00 - INFO - ============================================================
2026-02-25 10:20:00 - INFO -   BUILD INDEX V3 - HAProxy RAG (qwen3-embedding:8b)
2026-02-25 10:20:00 - INFO - ============================================================
2026-02-25 10:20:00 - INFO -
ğŸ“¦ 3645 chunks Ã  indexer
2026-02-25 10:20:00 - INFO - â™»ï¸  Index existant trouvÃ© : 0 documents dÃ©jÃ  indexÃ©s
2026-02-25 10:20:00 - INFO - ğŸ”„ Index vide, reprise depuis le chunk #0
2026-02-25 10:20:00 - INFO -
ğŸ”¨ Index ChromaDB V3 (qwen3-embedding:8b)...
2026-02-25 10:20:00 - INFO -    ğŸ“ 0 chunks dÃ©jÃ  indexÃ©s, 3645 restants
2026-02-25 10:20:00 - INFO -    ğŸ“¦ 37 batches de 100 chunks
2026-02-25 10:20:00 - INFO -    â±ï¸  Temps estimÃ©: ~74-148 min (qwen3-embedding:8b est lent)
...
2026-02-25 12:31:22 - INFO -    [ 3645/3645] 100.0% - ETA:   0.0 min
2026-02-25 12:31:22 - INFO - âœ… 3645 documents indexÃ©s (V3)
2026-02-25 12:31:22 - INFO -
ğŸ”¨ Index BM25 V3...
2026-02-25 12:31:23 - INFO - âœ… BM25 V3 crÃ©Ã© (3645 chunks)
2026-02-25 12:31:23 - INFO -
ğŸ“¦ Metadata V3...
2026-02-25 12:31:23 - INFO - âœ… 3645 chunks sauvegardÃ©s
2026-02-25 12:31:23 - INFO -
============================================================
2026-02-25 12:31:23 - INFO -   INDEX V3 CONSTRUIT EN 135.9 MINUTES
2026-02-25 12:31:23 - INFO - ============================================================
2026-02-25 12:31:23 - INFO -   Embedding    : qwen3-embedding:8b
2026-02-25 12:31:23 - INFO -   Dimension    : 4096 (qwen3-embedding:8b)
2026-02-25 12:31:23 - INFO -   MTEB Score   : 70.58 (#1 mondial)
2026-02-25 12:31:23 - INFO -   Chunks       : 3645
2026-02-25 12:31:23 - INFO -   ChromaDB     : index_v3\chroma/
2026-02-25 12:31:23 - INFO -   BM25         : index_v3\bm25.pkl
2026-02-25 12:31:23 - INFO -   Metadata     : index_v3\chunks.pkl
2026-02-25 12:31:23 - INFO - ============================================================
```

### **Fichiers gÃ©nÃ©rÃ©s :**
```
index_v3/
â”œâ”€â”€ chroma/           # Index vectoriel ChromaDB
â”‚   â”œâ”€â”€ chroma.sqlite3
â”‚   â””â”€â”€ ...
â”œâ”€â”€ bm25.pkl          # Index lexical BM25
â””â”€â”€ chunks.pkl        # Metadata des chunks
```

### **DurÃ©e :** ~2 heures (135 min)

---

## Ã‰tape 4 : Lancer le chatbot

**Objectif :** Interface Gradio pour poser des questions.

### **Execution**

```bash
uv run python 04_app_v3.py
```

### **Ce que Ã§a fait :**
- Charge les index V3 (ChromaDB + BM25)
- Lance un serveur Gradio
- Interface web pour poser des questions
- Retrieval hybride (vectoriel + lexical + rerank)
- GÃ©nÃ©ration de rÃ©ponse avec `qwen3:latest`

### **Sortie attendue :**
```
2026-02-25 13:00:00 - INFO - Importation des modules V3 (qwen3-embedding:8b)...
2026-02-25 13:00:00 - INFO - âœ… Module retriever_v3 importÃ© avec succÃ¨s
2026-02-25 13:00:00 - INFO - âœ… Module llm importÃ© avec succÃ¨s
2026-02-25 13:00:00 - INFO - ModÃ¨le par dÃ©faut: qwen3:latest
2026-02-25 13:00:00 - INFO - Tentative de chargement des index V3...
2026-02-25 13:00:01 - INFO - âœ… Index V3 chargÃ©s avec succÃ¨s (qwen3-embedding:8b)
2026-02-25 13:00:01 - INFO - Starting Gradio server...
* Running on local URL: http://localhost:7860
```

### **Interface :**
- **URL :** http://localhost:7860
- **FonctionnalitÃ©s :**
  - Chat avec historique
  - Affichage des sources
  - Exemples de questions
  - Toggle "Montrer les sources"

### **Exemples de questions :**
- "Comment configurer un health check HTTP ?"
- "Syntaxe de la directive bind ?"
- "Comment limiter les connexions par IP ?"
- "Comment utiliser les ACLs ?"
- "Options de timeout disponibles ?"

---

## Ã‰tape 5 : Benchmarker

**Objectif :** Mesurer la performance du RAG.

### **Niveaux de benchmark**

#### **Quick (7 questions, ~3 min)**
```bash
uv run python bench_v3_only.py --level quick
```

#### **Standard (20 questions, ~8 min)**
```bash
uv run python bench_v3_only.py --level standard
```

#### **Full (100 questions, ~45 min)**
```bash
uv run python bench_v3_only.py --level full
```

### **Benchmark ciblÃ© (par catÃ©gorie)**
```bash
# Tester backend + acl uniquement
uv run python bench_v3_targeted.py --categories backend,acl

# Tester des questions spÃ©cifiques
uv run python bench_v3_targeted.py --questions full_backend_name,full_acl_status
```

### **Comparaison V2 vs V3**
```bash
uv run python bench_v2_vs_v3.py --model qwen3:latest
```

### **Benchmark des modÃ¨les LLM**
```bash
uv run python bench_ollama_models.py
```

### **RÃ©sultats attendus (Full 100 questions) :**
```
======================================================================
ğŸ“ˆ RÃ‰SULTATS BENCHMARK V3
======================================================================

ğŸ¯ ModÃ¨le LLM: qwen3:latest
ğŸ“ Questions: 100
   Index: index_v3/ (qwen3-embedding:8b, MTEB 70.58)

----------------------------------------------------------------------
MÃ©trique                       | Valeur
----------------------------------------------------------------------
QualitÃ© moyenne                | 0.846          /1.0
Taux de rÃ©ussite (>0.7)        | 82.0           %
Questions rÃ©solues             : 82/100
Temps de retrieval moy.        : 6.84           s
Temps de gÃ©nÃ©ration moy.       : 15.58          s
Temps total                    : 2241.90        s
Tokens moy.                    : 504.1
----------------------------------------------------------------------

======================================================================
ğŸ’¡ INTERPRÃ‰TATION
======================================================================
âœ… TRÃˆS BON - QualitÃ© >= 0.80/1.0
âœ… 82.0% des questions rÃ©solues (objectif >= 80%)
```

---

## ğŸ—ï¸ Architecture technique

### **Pipeline de retrieval**

```
Question utilisateur
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Query Expansion    â”‚  Ajoute synonymes techniques
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ChromaDB Search    â”‚  Top-50 vectoriel (cosine)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BM25 Search        â”‚  Top-50 lexical
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RRF Fusion         â”‚  Combine les 2 scores
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FlashRank Rerank   â”‚  Top-10 avec cross-encoder
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Metadata Filtering â”‚  Filtre par section (optionnel)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Keyword Boosting   â”‚  Booste les chunks avec keywords
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM Generation     â”‚  qwen3:latest avec contexte
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
RÃ©ponse finale avec sources
```

### **Configurations clÃ©s**

#### **`retriever_v3.py`**
```python
TOP_K_RETRIEVAL      = 50  # Candidats par mÃ©thode
TOP_K_RRF            = 30  # AprÃ¨s fusion RRF
TOP_K_RERANK         = 10  # AprÃ¨s reranking
RRF_K                = 60  # ParamÃ¨tre RRF
CONFIDENCE_THRESHOLD = 0.0  # Seuil de confiance
```

#### **Metadata Filtering**
```python
SECTION_HINTS = {
    "stick-table": ["11.1", "11.2", "7.3", "11.3"],
    "acl": ["7.1", "7.2", "7.3", "7.4", "7.5", "8.1", "8.2"],
    "backend": ["5.1", "5.2", "5.3", "4.1", "4.3", "3.1"],
    "ssl": ["4.2", "5.1", "5.3", "3.1", "4.1"],
    ...
}
```

---

## ğŸ”§ DÃ©pannage

### **ProblÃ¨me : Ollama inaccessible**
```bash
# VÃ©rifier qu'Ollama tourne
ollama serve

# VÃ©rifier les modÃ¨les
ollama list

# RÃ©installer un modÃ¨le
ollama rm qwen3-embedding:8b
ollama pull qwen3-embedding:8b
```

### **ProblÃ¨me : Index manquants**
```bash
# Reconstruire l'index
uv run python 03_build_index_v3.py

# Ou supprimer et reconstruire
rm -rf index_v3/
uv run python 03_build_index_v3.py
```

### **ProblÃ¨me : ChromaDB error**
```bash
# Supprimer le cache ChromaDB
rm -rf index_v3/chroma/chroma.sqlite3

# Reconstruire
uv run python 03_build_index_v3.py
```

### **ProblÃ¨me : Gradio ne dÃ©marre pas**
```bash
# VÃ©rifier les dÃ©pendances
uv sync

# RÃ©installer Gradio
uv add gradio
```

### **ProblÃ¨me : QualitÃ© faible**
```bash
# VÃ©rifier le retrieval
uv run python bench_v3_targeted.py --categories backend,acl --verbose

# Ajuster SECTION_HINTS dans retriever_v3.py
# Reconstruire l'index si nÃ©cessaire
```

---

## ğŸ“Š Performance attendue

| MÃ©trique | Valeur | Objectif |
|----------|--------|----------|
| QualitÃ© moyenne | 0.846/1.0 | 0.80+ âœ… |
| Questions rÃ©solues | 82% | 80%+ âœ… |
| Temps/requÃªte | 22.4s | <25s âœ… |
| Chunks indexÃ©s | 3645 | - |
| Taille index | ~500 MB | - |

---

## ğŸ“ Structure des fichiers

```
haproxy-dataset-generator/
â”œâ”€â”€ 01_scrape.py              # Scrapping â†’ sections.jsonl
â”œâ”€â”€ 02_ingest_v2.py           # Chunking â†’ chunks_v3.jsonl
â”œâ”€â”€ 03_build_index_v3.py      # Indexing â†’ index_v3/
â”œâ”€â”€ 04_app_v3.py              # Chatbot Gradio
â”œâ”€â”€ retriever_v3.py           # Retrieval hybride V3
â”œâ”€â”€ llm.py                    # GÃ©nÃ©ration LLM
â”œâ”€â”€ bench_questions.py        # 100 questions de benchmark
â”œâ”€â”€ bench_v3_only.py          # Benchmark V3 (quick/standard/full)
â”œâ”€â”€ bench_v3_targeted.py      # Benchmark ciblÃ©
â”œâ”€â”€ bench_v2_vs_v3.py         # Comparaison V2 vs V3
â”œâ”€â”€ bench_ollama_models.py    # Benchmark modÃ¨les LLM
â”œâ”€â”€ V3_PERFORMANCE_TRACKING.md# Historique des perfs
â”œâ”€â”€ data/                     # DonnÃ©es brutes
â”‚   â”œâ”€â”€ sections.jsonl
â”‚   â””â”€â”€ chunks_v3.jsonl
â””â”€â”€ index_v3/                 # Index construits
    â”œâ”€â”€ chroma/
    â”œâ”€â”€ bm25.pkl
    â””â”€â”€ chunks.pkl
```

---

## ğŸš€ Commandes rapides

```bash
# Installation
uv sync
ollama pull qwen3-embedding:8b
ollama pull qwen3:latest

# Scrapping
uv run python 01_scrape.py

# Chunking
uv run python 02_ingest_v2.py

# Indexing (~2h)
uv run python 03_build_index_v3.py

# Chatbot
uv run python 04_app_v3.py

# Benchmark Quick (3 min)
uv run python bench_v3_only.py --level quick

# Benchmark Full (45 min)
uv run python bench_v3_only.py --level full
```

---

## ğŸ“š Ressources

- [HAProxy 3.2 Docs](https://docs.haproxy.org/3.2/)
- [Ollama](https://ollama.com)
- [ChromaDB](https://docs.trychroma.com/)
- [FlashRank](https://github.com/PrithivirajDamodaran/FlashRank)
- [Qwen3 Embedding](https://ollama.com/library/qwen3-embedding:8b)

---

**DerniÃ¨re mise Ã  jour :** 2026-02-25  
**Version :** V3 (qwen3-embedding:8b, MTEB 70.58)  
**Statut :** âœ… PrÃªt pour production
