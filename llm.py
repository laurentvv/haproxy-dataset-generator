"""
llm.py - GÃ©nÃ©ration de rÃ©ponses via Ollama avec streaming
Utilise le contexte rÃ©cupÃ©rÃ© par le retriever pour rÃ©pondre avec prÃ©cision.
"""

import os
import requests
from datetime import datetime, timedelta
import time
from typing import Generator
import logging

logger = logging.getLogger(__name__)


# â”€â”€ HTTP Session Pooling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Session globale pour le pooling de connexions HTTP
_llm_session = requests.Session()
_llm_session.mount(
    "http://",
    requests.adapters.HTTPAdapter(
        pool_connections=10,
        pool_maxsize=10,
        max_retries=3,
    ),
)


# â”€â”€ Rate Limiting â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class RateLimiter:
    """Rate limiter for API calls."""

    def __init__(self, calls_per_minute: int = 20):
        self.min_interval = timedelta(seconds=60 / calls_per_minute)
        self.last_call = datetime.min

    def wait_if_needed(self):
        now = datetime.now()
        elapsed = now - self.last_call
        if elapsed < self.min_interval:
            wait_time = (self.min_interval - elapsed).total_seconds()
            if wait_time > 0:
                time.sleep(wait_time)
        self.last_call = datetime.now()


_llm_limiter = RateLimiter(calls_per_minute=20)


# â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OLLAMA_URL = "http://localhost:11434"
DEFAULT_MODEL = "gemma3:latest"  # ChangÃ© de qwen3:latest Ã  gemma3:latest (plus stable)
MAX_CONTEXT_CHARS = 4000  # Limite de contexte envoyÃ© au LLM
LLM_TIMEOUT = int(
    os.getenv("LLM_TIMEOUT", "300")
)  # Timeout configurable (dÃ©faut: 300s)


# â”€â”€ Prompt systÃ¨me â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SYSTEM_PROMPT = """Tu es un assistant expert en HAProxy 3.2, spÃ©cialisÃ© dans la configuration et l'administration.

RÃˆGLES ABSOLUES :
1. RÃ©ponds UNIQUEMENT Ã  partir du contexte entre <context> et </context>
2. Si l'information n'est PAS dans le contexte â†’ dis "Cette information n'est pas dans la documentation fournie"
3. JAMAIS d'invention, JAMAIS de suppositions, JAMAIS de valeurs par dÃ©faut non documentÃ©es
4. Cite TOUJOURS la source entre parenthÃ¨ses : (Source: nom_de_la_section)
5. Pour les exemples de configuration, utilise des blocs de code avec la syntaxe haproxy
6. RÃ©ponds en franÃ§ais si la question est en franÃ§ais, en anglais si en anglais

STRUCTURE OBLIGATOIRE :
1. RÃ©ponse directe (1-2 phrases)
2. DÃ©tails techniques (paramÃ¨tres, options importantes)
3. Exemple de configuration (si pertinent dans le contexte)
4. Sources utilisÃ©es entre parenthÃ¨ses

EXEMPLE DE BONNE RÃ‰PONSE :
Question: "Comment configurer un health check HTTP ?"
RÃ©ponse:
"Pour configurer un health check HTTP, utilisez l'option `option httpchk` dans le backend.

Syntaxe: `option httpchk [<method> <uri> [<version>]]`

Exemple:
```
backend web_servers
    option httpchk GET /health HTTP/1.1
    server web1 192.168.1.1:80 check
```

(Source: 5.2. Server and default-server options)"""


PROMPT_TEMPLATE = """<context>
{context}
</context>

Question : {question}"""


FALLBACK_RESPONSE = """âš ï¸ Je n'ai pas trouvÃ© d'information suffisamment prÃ©cise dans la documentation HAProxy pour rÃ©pondre Ã  cette question.

Suggestions :
- Reformule ta question en utilisant des termes techniques HAProxy prÃ©cis (ex: "option httpchk", "backend", "ACL")
- Consulte directement la documentation : https://docs.haproxy.org/3.2/
- VÃ©rifie que le terme recherchÃ© existe dans HAProxy 3.2"""


def list_ollama_models() -> list[str]:
    """Retourne la liste des modÃ¨les Ollama disponibles, filtrÃ©e.

    Exclut les modÃ¨les d'embedding et les modÃ¨les vision-language (vl).

    Returns:
        Liste des noms de modÃ¨les disponibles
    """
    try:
        response = _llm_session.get(f"{OLLAMA_URL}/api/tags", timeout=5)
        response.raise_for_status()
        all_models = [m["name"] for m in response.json().get("models", [])]

        # Filtrer les modÃ¨les d'embedding et vision-language
        filtered_models = [
            model
            for model in all_models
            if "embed" not in model.lower() and "vl" not in model.lower()
        ]

        return filtered_models
    except Exception:
        return []


def truncate_context(context: str, max_chars: int = MAX_CONTEXT_CHARS) -> str:
    """
    Tronque le contexte si trop long en respectant les sÃ©parateurs --- (limites de chunks).
    """
    if len(context) <= max_chars:
        return context

    # Couper aux sÃ©parateurs de chunks
    parts = context.split("\n\n---\n\n")
    result = []
    total = 0

    for part in parts:
        if total + len(part) > max_chars:
            # Inclure au moins le premier chunk mÃªme s'il dÃ©passe
            if not result:
                result.append(part[:max_chars])
            break
        result.append(part)
        total += len(part)

    truncated = "\n\n---\n\n".join(result)
    if len(truncated) < len(context):
        truncated += f"\n\n[... contexte tronquÃ© Ã  {max_chars} caractÃ¨res ...]"

    return truncated


def build_messages(
    question: str, context: str, history: list[tuple[str, str]] | None = None
) -> list[dict]:
    """
    Construit la liste de messages pour l'API Ollama.

    Args:
        question : Question actuelle
        context  : Contexte rÃ©cupÃ©rÃ© par le retriever
        history  : Historique [(question, rÃ©ponse), ...] des 3 derniers tours max
    """
    messages = [{"role": "system", "content": SYSTEM_PROMPT}]

    # Ajouter l'historique de conversation (max 3 tours)
    if history:
        for user_msg, assistant_msg in history[-3:]:
            messages.append({"role": "user", "content": user_msg})
            messages.append({"role": "assistant", "content": assistant_msg})

    # Message utilisateur avec contexte
    user_content = PROMPT_TEMPLATE.format(
        context=truncate_context(context), question=question
    )
    messages.append({"role": "user", "content": user_content})

    return messages


def generate_response(
    question: str,
    context: str,
    model: str = DEFAULT_MODEL,
    history: list[tuple[str, str]] | None = None,
    temperature: float = 0.1,  # Faible pour rester factuel
) -> Generator[str, None, None]:
    """
    GÃ©nÃ¨re une rÃ©ponse en streaming.

    Yields:
        Tokens de la rÃ©ponse au fur et Ã  mesure
    """
    import json  # Pour le parsing du streaming

    # Wait if needed to respect rate limit
    _llm_limiter.wait_if_needed()

    messages = build_messages(question, context, history)

    # DEBUG: Afficher les messages
    logger.debug("Messages envoyÃ©s Ã  Ollama:")
    for msg in messages:
        content_preview = msg["content"][:100].replace("\n", " ")
        logger.debug("  - %s: %s...", msg["role"], content_preview)
    logger.debug("ModÃ¨le: %s, Temperature: %s", model, temperature)

    # DÃ©tecter si c'est un modÃ¨le GGUF (qui nÃ©cessite un format diffÃ©rent)
    is_gguf = "GGUF" in model or "gguf" in model.lower()

    # Pour les modÃ¨les GGUF, construire un prompt simple au lieu de messages
    if is_gguf:
        # Construire un prompt simple pour les modÃ¨les GGUF
        prompt_parts = []
        for msg in messages:
            role = msg["role"]
            content = msg["content"]
            if role == "system":
                prompt_parts.append(f"System: {content}\n")
            elif role == "user":
                prompt_parts.append(f"User: {content}\n")
            elif role == "assistant":
                prompt_parts.append(f"Assistant: {content}\n")
        prompt_parts.append("Assistant:")  # Ajouter le dÃ©but de la rÃ©ponse
        prompt = "".join(prompt_parts)

        payload = {
            "model": model,
            "prompt": prompt,
            "stream": True,
            "options": {
                "temperature": temperature,
                "top_p": 0.9,
                "repeat_penalty": 1.1,
                "num_predict": 1024,
            },
            "keep_alive": "5m",  # Keep model loaded for 5 minutes
        }
        endpoint = f"{OLLAMA_URL}/api/generate"
    else:
        # Format standard pour les modÃ¨les natifs Ollama
        payload = {
            "model": model,
            "messages": messages,
            "stream": True,
            "options": {
                "temperature": temperature,
                "top_p": 0.9,
                "repeat_penalty": 1.1,
                "num_predict": 1024,
            },
            "keep_alive": "5m",  # Keep model loaded for 5 minutes
        }
        endpoint = f"{OLLAMA_URL}/api/chat"

    try:
        with _llm_session.post(
            endpoint, json=payload, stream=True, timeout=LLM_TIMEOUT
        ) as response:
            response.raise_for_status()

            for line in response.iter_lines():
                if line:
                    try:
                        data = json.loads(line.decode("utf-8"))
                        # Les deux endpoints retournent des formats lÃ©gÃ¨rement diffÃ©rents
                        if is_gguf:
                            token = data.get("response", "")
                        else:
                            token = data.get("message", {}).get("content", "")
                        if token:
                            yield token
                        if data.get("done"):
                            break
                    except json.JSONDecodeError:
                        continue

    except requests.exceptions.ConnectionError:
        yield f"âŒ Impossible de se connecter Ã  Ollama sur {OLLAMA_URL}.\nVÃ©rifie qu'Ollama tourne : `ollama serve`"
    except requests.exceptions.Timeout:
        yield "â±ï¸ Timeout â€” le modÃ¨le met trop de temps Ã  rÃ©pondre. Essaie un modÃ¨le plus lÃ©ger."
    except requests.exceptions.HTTPError as e:
        if "404" in str(e):
            yield f"âŒ ModÃ¨le '{model}' non trouvÃ©.\nInstalle-le : `ollama pull {model}`"
        else:
            yield f"âŒ Erreur Ollama : {e}"
    except Exception as e:
        yield f"âŒ Erreur inattendue : {e}"


def generate_response_sync(
    question: str,
    context: str,
    model: str = DEFAULT_MODEL,
    history: list[tuple[str, str]] | None = None,
) -> str:
    """Version synchrone (non-streaming) â€” utile pour les tests."""
    return "".join(generate_response(question, context, model, history))


# â”€â”€ Test CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    import sys

    # Test simple sans RAG
    question = (
        sys.argv[1]
        if len(sys.argv) > 1
        else "Comment configurer un backend avec health check HTTP ?"
    )
    context = """[Source 1: option httpchk â€” https://docs.haproxy.org/3.2/configuration.html]

option httpchk
    Use HTTP protocol to check server health

    Syntax: option httpchk [<method> <uri> [<version>]]
    
    Example:
    ```haproxy
    backend web_servers
        option httpchk GET /health HTTP/1.1\\r\\nHost:\\ example.com
        server web1 192.168.1.1:80 check
    ```
    """

    print(f"ğŸ¤– Question : {question}\n")
    print("ğŸ“ RÃ©ponse :\n")

    models = list_ollama_models()
    model = (
        DEFAULT_MODEL
        if DEFAULT_MODEL in models
        else (models[0] if models else DEFAULT_MODEL)
    )

    for token in generate_response(question, context, model=model):
        print(token, end="", flush=True)
    print("\n")
