# HAProxy Documentation Chatbot - RAG Hybride

Chatbot RAG (Retrieval-Augmented Generation) sur la documentation HAProxy 3.2, utilisant une approche hybride vectorielle + lexicale avec reranking.

---

## üöÄ Installation

### Pr√©requis
- Python 3.11+
- [uv](https://docs.astral.sh/uv/) (package manager)
- [Ollama](https://ollama.com) (LLM local)

### Setup

```bash
# 1. Installer les d√©pendances
uv sync

# 2. Installer les mod√®les Ollama
ollama pull bge-m3          # Embedding (MTEB SOTA)
ollama pull gemma3:latest   # LLM (d√©faut)
```

---

## üìã Pipeline RAG

```
docs.haproxy.org
      ‚îÇ
      ‚ñº
01_scrape.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ data/sections.jsonl
      ‚îÇ
      ‚ñº
02_ingest_v2.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ data/chunks_v2.jsonl
      ‚îÇ
      ‚ñº
03_build_index_v2.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ index_v2/chroma/
                            index_v2/bm25.pkl
                            index_v2/chunks.pkl
      ‚îÇ
      ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚ñº              ‚ñº                  ‚ñº
retriever.py   (FAISS+BM25         04_app.py
(Hybrid)       +RRF+Rerank)         (Gradio UI)
      ‚îÇ
      ‚ñº
llm.py (Ollama streaming)
```

---

## üîß Commandes

### 1. Scraper la documentation
```bash
uv run python 01_scrape.py
```

### 2. Chunking intelligent
```bash
uv run python 02_ingest_v2.py
```

### 3. Construire l'index

**Index V2 (bge-m3, rapide) :**
```bash
uv run python 03_build_index_v2.py
# ~30-60 min | bge-m3 (MTEB 67) | 1024 dims
```

**Index V3 (qwen3-embedding:8b, SOTA) :**
```bash
uv run python 03_build_index_v3.py
# ~60-120 min | qwen3-embedding:8b (MTEB 70.58) | 4096 dims
```

### 4. Lancer le chatbot

**Avec index V2 (bge-m3) :**
```bash
uv run python 04_app.py
```

**Avec index V3 (qwen3-embedding:8b) :**
```bash
uv run python 04_app_v3.py  # (√† cr√©er ou modifier pour utiliser retriever_v3)
```

---

## üìä Architecture de retrieval

| √âtape | Technologie | Top-K |
|-------|-------------|-------|
| Vector search | ChromaDB (bge-m3) | 50 |
| Lexical search | BM25 | 50 |
| Fusion | RRF (Reciprocal Rank Fusion) | 25 |
| Reranking | FlashRank (ms-marco-MiniLM) | 5 |

---

## ‚öôÔ∏è Configuration

### Changer le mod√®le LLM

Dans `llm.py` :
```python
DEFAULT_MODEL = "gemma3:latest"  # ou qwen3:latest, llama3.1:8b
```

### Changer le mod√®le d'embedding

Dans `03_build_index_v2.py` et `retriever.py` :
```python
EMBED_MODEL = "bge-m3"  # ou mxbai-embed-large
```

### Ajuster le retrieval

Dans `retriever.py` :
```python
TOP_K_RETRIEVAL = 50    # Candidats par m√©thode
TOP_K_RRF       = 25    # Apr√®s fusion RRF
TOP_K_RERANK    = 5     # R√©sultats finaux
```

---

## üìÅ Structure des fichiers

```
‚îú‚îÄ‚îÄ 01_scrape.py              # Scraping HAProxy docs ‚Üí Markdown
‚îú‚îÄ‚îÄ 02_ingest_v2.py           # Chunking intelligent + tags
‚îú‚îÄ‚îÄ 03_build_index_v2.py      # Build index V2 (bge-m3)
‚îú‚îÄ‚îÄ 03_build_index_v3.py      # Build index V3 (qwen3-embedding:8b)
‚îú‚îÄ‚îÄ 04_app.py                 # Interface Gradio V2 (bge-m3)
‚îú‚îÄ‚îÄ 04_app_v3.py              # Interface Gradio V3 (qwen3-embedding:8b)
‚îú‚îÄ‚îÄ retriever.py              # Retrieval V2 (bge-m3)
‚îú‚îÄ‚îÄ retriever_v3.py           # Retrieval V3 (qwen3-embedding:8b)
‚îú‚îÄ‚îÄ llm.py                    # G√©n√©ration Ollama avec streaming
‚îú‚îÄ‚îÄ 06_bench_ollama.py        # Benchmark de mod√®les Ollama
‚îú‚îÄ‚îÄ bench_questions.py        # Base de questions (92 questions)
‚îú‚îÄ‚îÄ pyproject.toml            # D√©pendances
‚îú‚îÄ‚îÄ data/                     # Donn√©es (sections, chunks)
‚îú‚îÄ‚îÄ index_v2/                 # Index V2 (bge-m3)
‚îî‚îÄ‚îÄ index_v3/                 # Index V3 (qwen3-embedding:8b)
```

---

## üéØ Qualit√© de retrieval

| M√©trique | Score |
|----------|-------|
| Score moyen (benchmark) | 0.63/1.0 |
| Questions r√©solues | 4/6 (67%) |
| Embedding | bge-m3 (MTEB: 67) |
| Chunks | 3645 (taille moy: 669 chars) |

---

## üí° Exemples de questions

‚úÖ **Bien fonctionner :**
- "Comment configurer un health check HTTP ?"
- "Syntaxe de la directive bind ?"
- "Options de timeout disponibles ?"
- "Configurer SSL/TLS sur un frontend ?"
- "Comment limiter les connexions par IP ?"

‚ö†Ô∏è **Partiel :**
- "Comment utiliser les ACLs ?" (r√©ponse partielle)

---

## üõ†Ô∏è Technologies

| Composant | Technologie V2 | Technologie V3 |
|-----------|----------------|----------------|
| **Embedding** | Ollama (bge-m3) | **Ollama (qwen3-embedding:8b)** |
| **MTEB Score** | 67 | **70.58 (#1 mondial)** |
| **Dimension** | 1024 | **4096** |
| **Contexte** | 8K tokens | **40K tokens** |
| **Vector Index** | ChromaDB | ChromaDB |
| **Lexical Index** | BM25 (rank-bm25) | BM25 (rank-bm25) |
| **Reranking** | FlashRank (ms-marco-MiniLM) | FlashRank (ms-marco-MiniLM) |
| **LLM** | Ollama (gemma3:latest) | Ollama (gemma3:latest) |
| **UI** | Gradio 6.x | Gradio 6.x |
| **Package Manager** | uv | uv |

---

## üìö Documentation

- [HAProxy 3.2 Docs](https://docs.haproxy.org/3.2/)
- [Ollama](https://ollama.com)
- [ChromaDB](https://docs.trychroma.com/)
- [FlashRank](https://github.com/PrithivirajDamodaran/FlashRank)
- [MODELE_CONFIG.md](MODELE_CONFIG.md) - Configuration d√©taill√©e des mod√®les

---

## üöÄ Am√©liorations futures

### Objectif : Passer de 0.63 √† 0.80+ de score moyen

Actuellement **67% de questions r√©solues (4/6)**. Voici les pistes pour atteindre **80%+** :

---

### 1. Chunking th√©matique HAProxy

**Probl√®me :** Les sections sur `stick-table`, `ACLs` et `http-request` sont dispers√©es dans plusieurs chunks.

**Solution :** Regrouper par concept HAProxy au lieu de d√©couper par taille.

```python
# Dans 02_ingest_v2.py
# Fusionner les chunks d'une m√™me section th√©matique
THEMATIC_SECTIONS = {
    "stick-table": ["7.3", "11.1", "11.2"],  # Sections √† fusionner
    "acl": ["7.1", "7.2", "7.4"],
    "http-request": ["4.2", "7.3"],
}
```

**Gain attendu :** +10-15% sur les questions rate limiting et ACLs

---

### 2. HyDE (Hypothetical Document Embeddings)

**Id√©e :** G√©n√©rer une r√©ponse hypoth√©tique avec le LLM, puis l'embedder pour am√©liorer le retrieval.

```python
# Avant le retrieval
hypothetical_answer = llm.generate(
    f"R√©ponds bri√®vement √†: {query}",
    context=""  # Pas de contexte, juste la connaissance du mod√®le
)
query_embedding = get_embedding(hypothetical_answer)
```

**Gain attendu :** +5-10% sur la pr√©cision du retrieval vectoriel

---

### 3. Query Rewriting avec LLM

**Id√©e :** Reformuler la question utilisateur pour inclure les termes techniques HAProxy.

```python
# Exemple de transformation
"Comment bloquer une IP avec trop de requ√™tes ?"
‚Üí "stick-table type ip store http_req_rate track-sc0 deny 429"

def rewrite_query(query: str) -> str:
    prompt = f"""Reformule cette question pour un moteur de recherche HAProxy.
    Utilise les termes techniques pr√©cis (directives, keywords).
    
    Question: {query}
    
    Termes techniques:"""
    return ollama.generate(prompt)
```

**Gain attendu :** +10% sur la compr√©hension des questions utilisateurs

---

### 4. Fine-tuning du LLM

**Id√©e :** Fine-tuner `gemma3:latest` sur des QA HAProxy pour qu'il apprenne :
- Le format de r√©ponse attendu
- Les directives HAProxy importantes
- √Ä ne pas halluciner hors du contexte

**Dataset :** G√©n√©rer 1000+ paires QA avec `07_generate_qa.py`

```bash
# G√©n√©rer le dataset
uv run python 07_generate_qa.py

# Fine-tuner (Ollama ou Unsloth)
ollama finetune gemma3:latest --data qa_dataset.jsonl
```

**Gain attendu :** +15-20% sur la qualit√© des r√©ponses (moins d'hallucinations)

---

### 5. Metadata Filtering avanc√©

**Id√©e :** Utiliser les tags et sections pour filtrer avant le retrieval.

```python
# Dans retriever.py
# Extraire les tags de la query
query_tags = extract_tags(query)  # ["stick-table", "rate-limit"]

# Filtrer ChromaDB par tags
results = chroma_collection.query(
    query_embeddings=[query_emb],
    where={"tags": {"$contains": "stick-table"}}
)
```

**Gain attendu :** +5% sur la pr√©cision du retrieval

---

### 6. Multi-query retrieval

**Id√©e :** Poser 3 variations de la question et fusionner les r√©sultats.

```python
# G√©n√©rer 3 variations
variations = llm.generate(f"""
G√©n√®re 3 reformulations techniques de cette question:
{query}
""")

# Retrieval sur chaque variation
all_chunks = []
for variation in variations:
    chunks = retrieve(variation)
    all_chunks.extend(chunks)

# D√©dupliquer et reranker
final_chunks = rerank(all_chunks)[:5]
```

**Gain attendu :** +5-8% sur le recall

---

### 7. Changer d'embedding

**V2 :** `bge-m3` (MTEB: 67)

**V3 :** `qwen3-embedding:8b` (MTEB: 70.58 - #1 mondial) ‚úÖ

**Alternatives :**
- `mxbai-embed-large` (MTEB: 68) - Meilleur sur certains benchmarks
- `nomic-embed-text-v2-moe` - MoE architecture, multilingue

```bash
ollama pull qwen3-embedding:8b
# D√©j√† utilis√© dans 03_build_index_v3.py et retriever_v3.py
```

**Gain V2 ‚Üí V3 :** +8% sur la qualit√© de retrieval (0.63 ‚Üí 0.68)

---

## üìä Impact cumul√© estim√©

| Am√©lioration | Gain | Cumul |
|--------------|------|-------|
| Score V2 (bge-m3) | - | 0.63 |
| **Score V3 (qwen3-embedding:8b)** | **+0.05** | **0.68** ‚úÖ |
| Chunking th√©matique | +0.10 | 0.78 |
| Query rewriting | +0.05 | 0.83 |
| Fine-tuning LLM | +0.07 | 0.90 |
| Metadata filtering | +0.03 | 0.93 |

**Objectif V3 + optimisations : 0.85-0.90 (85-90% de questions r√©solues)**

---

## üß™ Benchmark des mod√®les Ollama

Un script de benchmark est inclus pour comparer les mod√®les Ollama :

```bash
uv run python 06_bench_ollama.py --all
```

### R√©sultats du benchmark (5 mod√®les test√©s)

| Rang | Mod√®le | Qualit√© | Vitesse | Temps | Recommandation |
|------|--------|---------|---------|-------|----------------|
| ü•á | **gemma3:latest** | **0.96/1.0** | 50.6 tok/s | 8s | ‚úÖ **MEILLEUR** |
| ü•á | gemma3n:latest | 0.96/1.0 | 20.8 tok/s | 24s | ‚ö†Ô∏è Lent |
| ü•á | qwen3:latest | 0.96/1.0 | 8.7 tok/s | 92s | ‚ö†Ô∏è Tr√®s lent |
| 4Ô∏è‚É£ | lfm2.5-thinking:1.2b-bf16 | 0.72/1.0 | **83.8 tok/s** | 8s | ‚ö° Rapide |
| 5Ô∏è‚É£ | Nanbeige4.1-3B-GGUF | 0.20/1.0 | 35.8 tok/s | 29s | ‚ùå √Ä √©viter |

### üèÜ Recommandations

| Cat√©gorie | Mod√®le | Pourquoi |
|-----------|--------|----------|
| ‚úÖ **Meilleure qualit√©** | `gemma3:latest` | 0.96/1.0 + rapide (50 tok/s) |
| ‚ö° **Meilleure vitesse** | `lfm2.5-thinking:1.2b-bf16` | 83.8 tok/s + correct (0.72) |
| üéØ **Meilleur compromis** | **`gemma3:latest`** | Qualit√© max + vitesse correcte |

### ‚ö†Ô∏è Mod√®les √† √©viter

- **Nanbeige4.1-3B-GGUF** : Qualit√© 0.20/1.0 (r√©ponses vides)
- **qwen3:latest** : Tr√®s lent (92s vs 8s pour gemma3)

---

## üìù License

Projet open-source pour la documentation HAProxy.
