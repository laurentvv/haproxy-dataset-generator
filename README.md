# HAProxy LLM Dataset Generator & Fine-tuning Pipeline

[![Python Version](https://img.shields.io/badge/python-3.13+-blue.svg)](https://python.org)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Ce projet fournit un pipeline complet pour cr√©er un dataset de haute qualit√© pour le fine-tuning de mod√®les de langage (LLM), en se basant sur la documentation officielle de HAProxy. L'objectif est de produire un mod√®le sp√©cialis√© dans les questions-r√©ponses li√©es √† la configuration, l'administration et l'utilisation d'HAProxy.

## üöÄ Fonctionnalit√©s

- **Extraction automatis√©e** de la documentation HAProxy
- **G√©n√©ration de Q/R** √† l'aide de mod√®les LLM locaux (Ollama)
- **Dataset enrichi** avec titres, contenus et contextes complets
- **Pipeline de fine-tuning** pr√™t pour Google Colab
- **Support de multiples mod√®les** pour le fine-tuning (Gemma, Llama, etc.)

## üìÅ Structure du projet

```
haproxy-dataset-generator/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ TODO.md
‚îú‚îÄ‚îÄ uv.lock
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ extract_to_markdown.py      # Extraction de la doc HAProxy
‚îÇ   ‚îî‚îÄ‚îÄ generate_qa_with_ollama.py  # G√©n√©ration des paires Q/R
‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îî‚îÄ‚îÄ finetune_haproxy_on_colab.ipynb  # Notebook pour fine-tuning
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ sections.jsonl              # Sections extraites de la doc
‚îÇ   ‚îî‚îÄ‚îÄ haproxy_dataset_qa.jsonl    # Dataset final Q/R enrichi
‚îî‚îÄ‚îÄ .env.example                    # Exemple de fichier de configuration
```

## üõ† Pr√©requis

- Python 3.13+
- [uv](https://github.com/astral-sh/uv) (gestionnaire de paquets Python)
- [Ollama](https://ollama.com/) (pour ex√©cuter les mod√®les LLM localement)
- Mod√®le `qwen3:14b` install√© via Ollama (pour la g√©n√©ration de Q/R)

### Installation des outils

#### uv
```bash
# Sur macOS et Linux
curl -LsSf https://astral.sh/uv/install.sh | sh

# Sur Windows (PowerShell)
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
```

#### Ollama
Suivez les instructions d'installation sur [ollama.com](https://ollama.com/), puis d√©marrez le service :
```bash
ollama serve
```

#### T√©l√©chargement du mod√®le
```bash
ollama pull qwen3:14b
```

## üõ† Installation

1. **Cloner le d√©p√¥t**
   ```bash
   git clone <URL_DU_DEPOT>
   cd haproxy-dataset-generator
   ```

2. **Cr√©er l'environnement virtuel avec uv**
   ```bash
   # Cr√©e un environnement virtuel dans un dossier .venv
   uv venv

   # Active l'environnement
   # Sur macOS et Linux :
   source .venv/bin/activate
   # Sur Windows (Command Prompt) :
   .venv\Scripts\activate
   # Sur Windows (PowerShell) :
   .venv\Scripts\Activate.ps1
   ```

3. **Installer les d√©pendances du projet**
   ```bash
   # Installe le projet en mode d√©veloppement avec toutes ses d√©pendances
   uv pip install -e .
   ```

4. **Configurer les variables d'environnement**
   ```bash
   # Copier le fichier exemple
   cp .env.example .env

   # Modifier les variables selon vos besoins (URL de la documentation HAProxy, etc.)
   ```

## üß± Utilisation

Le pipeline se compose de deux √©tapes principales suivies d'une √©tape de fine-tuning :

### 1. Extraction de la documentation HAProxy

```bash
python scripts/extract_to_markdown.py
```

Ce script :
- T√©l√©charge la documentation HAProxy √† partir de l'URL sp√©cifi√©e dans `.env`
- D√©coupe le contenu en sections (balises `<h2>`)
- Convertit chaque section en Markdown
- Sauvegarde les sections structur√©es dans `data/sections.jsonl`

### 2. G√©n√©ration du dataset Question/R√©ponse

```bash
python scripts/generate_qa_with_ollama.py
```

Ce script :
- Lit les sections extraites de `data/sections.jsonl`
- G√©n√®re des paires Question/R√©ponse √† l'aide du mod√®le `qwen3:14b`
- Sauvegarde les paires enrichies (avec `title`, `content`) dans `data/haproxy_dataset_qa.jsonl`

### 3. Fine-tuning du mod√®le

Le dataset g√©n√©r√© est pr√™t √† √™tre utilis√© pour le fine-tuning d'un mod√®le plus l√©ger et sp√©cialis√©. Le notebook `training/finetune_haproxy_on_colab.ipynb` vous guide √† travers le processus de fine-tuning sur Google Colab en utilisant QLoRA (4-bit quantization) et LoRA (Low-Rank Adaptation).

Le notebook inclut :
- Chargement du dataset g√©n√©r√©
- Configuration du mod√®le de base (Gemma-2-9b-it, Llama-3-8B-Instruct, etc.)
- Mise en place de la quantification 4-bit (QLoRA)
- Configuration de LoRA
- Entra√Ænement du mod√®le
- Sauvegarde du mod√®le fine-tun√©
- Test du mod√®le fine-tun√©

Pour utiliser le notebook :
1. T√©l√©chargez ou clonez le d√©p√¥t sur votre Google Drive
2. Ouvrez le fichier dans Google Colab
3. Suivez les instructions pas √† pas dans le notebook

## üìä Format du dataset

Le dataset final `data/haproxy_dataset_qa.jsonl` contient des objets JSON avec les champs suivants :
- `question`: La question g√©n√©r√©e par le LLM
- `response`: La r√©ponse d√©taill√©e g√©n√©r√©e par le LLM
- `source`: URL de la section d'origine
- `title`: Titre de la section d'origine
- `content`: Contenu de la section d'origine (format Markdown)

Exemple :
```json
{
  "question": "Quelle est la directive 'bind' dans HAProxy et comment l'utiliser ?",
  "response": "La directive 'bind' dans HAProxy est utilis√©e pour sp√©cifier l'adresse IP et le port sur lesquels le proxy √©coutera les connexions entrantes...",
  "source": "https://docs.haproxy.org/3.2/intro.html",
  "title": "3.1. What HAProxy is and isn't",
  "content": "HAProxy is a TCP proxy : it can accept a TCP connection from a listening socket..."
}
```

## ü§ù Contribution

Les contributions sont les bienvenues ! Voici comment vous pouvez contribuer :

1. Fork du projet
2. Cr√©ez une branche pour votre fonctionnalit√© (`git checkout -b feature/NouvelleFonctionnalite`)
3. Committez vos changements (`git commit -m 'Ajouter une nouvelle fonctionnalit√©'`)
4. Poussez vers la branche (`git push origin feature/NouvelleFonctionnalite`)
5. Ouvrez une Pull Request

## üìÑ Licence

Ce projet est distribu√© sous la licence MIT. Voir le fichier `LICENSE` pour plus de d√©tails.

## üôè Remerciements

- [HAProxy Technologies](https://www.haproxy.com/) pour la documentation ouverte
- [Ollama](https://ollama.com/) pour les mod√®les LLM accessibles localement
- [Google Colab](https://colab.research.google.com/) pour l'infrastructure de fine-tuning
- [Hugging Face](https://huggingface.co/) pour les biblioth√®ques de machine learning
