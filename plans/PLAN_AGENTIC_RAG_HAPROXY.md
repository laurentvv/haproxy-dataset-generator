# PLAN â€” Agentic RAG HAProxy avec ChromaDB + LangGraph
> **Pour agent de codage IA** â€” Pipeline numÃ©rotÃ©, zÃ©ro suppression du repo existant, dÃ©veloppement en parallÃ¨le dans `agentic_rag/`

---

## CONTEXTE & OBJECTIF

### Repos de rÃ©fÃ©rence
- **Source existante** : `https://github.com/laurentvv/haproxy-dataset-generator`
  - Pipeline RAG hybride HAProxy 3.2 (retriever_v3, Gradio, Ollama)
  - Stack : `uv`, `qwen3-embedding:8b`, `qwen3:latest`, ChromaDB implicite via retriever_v3
  - Pipeline numÃ©rotÃ© `00_rebuild_all.py` â†’ `07_bench_config_correction.py`
- **Architecture cible** : `https://github.com/GiovanniPasq/agentic-rag-for-dummies`
  - LangGraph + conversation memory + human-in-the-loop + parent/child indexing
  - Multi-Agent Map-Reduce pour questions complexes

### RÃ¨gle absolue
> **Ne jamais modifier, dÃ©placer ou supprimer de fichiers existants** dans le repo racine.
> Tout le nouveau code va dans `agentic_rag/`.

### Stack technique retenue
| Composant | Choix | Raison |
|---|---|---|
| Vector store | **ChromaDB** local | Simple, pas de serveur, suffisant pour ~500 pages HAProxy |
| LLM orchestration | **LangGraph** | Conversation memory + human-in-the-loop natif |
| LLM | **Ollama** (`qwen3:latest`) | CohÃ©rence avec repo existant |
| Embeddings | **Ollama** `qwen3-embedding:8b` | Identique au projet principal â€” mÃªme espace vectoriel, benchmarks comparables |
| Interface | **Gradio 6.6.0** | Copie du chatbot `app/` existant, adaptÃ© pour pointer sur ChromaDB agentic |
| Gestion deps | **uv** | CohÃ©rence avec repo existant |

---

## STRUCTURE CIBLE COMPLÃˆTE

```
agentic_rag/                            â† NOUVEAU rÃ©pertoire (ne touche Ã  rien d'existant)
â”‚
â”œâ”€â”€ 00_rebuild_agentic.py               â† Orchestrateur : lance tout Ã  la suite
â”œâ”€â”€ 01_scrape_verified.py               â† Scraping + analyse hiÃ©rarchie parent/child
â”œâ”€â”€ 02_chunking_parent_child.py         â† Chunking hiÃ©rarchique alignÃ© HTML HAProxy
â”œâ”€â”€ 03_indexing_chroma.py               â† Indexation ChromaDB (dense search + MMR)
â”œâ”€â”€ 04_agentic_chatbot.py               â† Chatbot LangGraph complet (Gradio)
â”œâ”€â”€ 05_bench_agentic.py                 â† Benchmark comparatif vs retriever_v3 existant
â”œâ”€â”€ 06_eval_parent_child.py             â† Ã‰valuation qualitÃ© stratÃ©gie parent/child
â”œâ”€â”€ 07_export_dataset_agentic.py        â† GÃ©nÃ©ration dataset Q&A enrichi (fine-tuning)
â”‚
â”œâ”€â”€ app/                                â† COPIE de app/ existant + adaptations retriever
â”‚   â”œâ”€â”€ gradio_app.py                   â† Titre modifiÃ© uniquement
â”‚   â”œâ”€â”€ chat_interface.py               â† Import rag_system modifiÃ© (1 ligne)
â”‚   â”œâ”€â”€ document_manager.py             â† Copie sans modification
â”‚   â””â”€â”€ rag_system.py                   â† RÃ‰Ã‰CRIT : AgenticRAGSystem wrapping agent_graph
â”‚
â”‚
â”œâ”€â”€ rag_agent/                          â† Module LangGraph
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ graph.py                        â† Construction et compilation du graphe
â”‚   â”œâ”€â”€ graph_state.py                  â† State TypedDict (MessagesState Ã©tendu)
â”‚   â”œâ”€â”€ nodes.py                        â† NÅ“uds : summarize, analyze_rewrite, agent_node
â”‚   â”œâ”€â”€ edges.py                        â† Routing conditionnel
â”‚   â”œâ”€â”€ tools.py                        â† search_child_chunks, retrieve_parent_chunks, validate_config
â”‚   â”œâ”€â”€ schemas.py                      â† Pydantic v2 : QueryAnalysis, etc.
â”‚   â””â”€â”€ prompts.py                      â† System prompts HAProxy-specific
â”‚
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ chroma_manager.py               â† Setup/reset ChromaDB
â”‚   â””â”€â”€ parent_store_manager.py         â† Lecture/Ã©criture JSON store
â”‚
â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ haproxy_scraper.py              â† Scraper dÃ©diÃ© HAProxy docs 3.2 (basÃ© sur 01_scrape.py existant)
â”‚   â”œâ”€â”€ html_structure_analyzer.py      â† Analyse structure HTML â†’ hiÃ©rarchie parent/child
â”‚   â””â”€â”€ compare_with_reference.py       â† Diff agentic vs projet principal (validation humaine)
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ conftest.py                     â† Fixtures pytest partagÃ©es
â”‚   â”œâ”€â”€ test_scraper_alignment.py       â† VÃ©rifie hiÃ©rarchie parent/child dans les donnÃ©es
â”‚   â”œâ”€â”€ test_chunking.py                â† VÃ©rifie tailles, overlap, parent_id linkage
â”‚   â”œâ”€â”€ test_retrieval.py               â† Teste search_child + retrieve_parent en isolÃ©
â”‚   â”œâ”€â”€ test_graph_flow.py              â† Teste transitions LangGraph
â”‚   â””â”€â”€ test_end_to_end.py              â† Tests E2E avec vraies questions HAProxy
â”‚
â”œâ”€â”€ data_agentic/                       â† DonnÃ©es gÃ©nÃ©rÃ©es (dans .gitignore)
â”‚   â”œâ”€â”€ scraped_pages.json
â”‚   â”œâ”€â”€ hierarchy_report.json
â”‚   â”œâ”€â”€ scraping_diff_report.json       â† Diff vs projet principal (validation Phase 1)
â”‚   â”œâ”€â”€ chunks_child.json               â† Dump de vÃ©rification des chunks
â”‚   â”œâ”€â”€ bench_comparison.json
â”‚   â”œâ”€â”€ parent_child_eval.json
â”‚   â””â”€â”€ pipeline_run.log
â”‚
â”œâ”€â”€ index_agentic/
â”‚   â””â”€â”€ chroma_db/                      â† Persistance ChromaDB (dans .gitignore)
â”‚
â”œâ”€â”€ parent_store/                       â† JSON des chunks parents (dans .gitignore)
â”‚
â”œâ”€â”€ pyproject_agentic.toml              â† DÃ©pendances isolÃ©es (uv)
â””â”€â”€ README_AGENTIC.md                   â† Instructions installation + usage
```

---

## INSTRUCTION PRÃ‰LIMINAIRE POUR L'AGENT â€” context7

> âš ï¸ **AVANT D'Ã‰CRIRE UNE SEULE LIGNE DE CODE**, l'agent doit interroger **context7** pour obtenir les APIs Ã  jour des bibliothÃ¨ques suivantes. Les APIs changent frÃ©quemment ; ne jamais coder de mÃ©moire.

```
# RequÃªtes context7 obligatoires (dans cet ordre) :
1. langgraph          â†’ MessagesState, StateGraph, ToolNode, tools_condition,
                        InMemorySaver, interrupt_before, add_node, add_edge,
                        add_conditional_edges, compile
2. langchain-core     â†’ tool decorator, SystemMessage, HumanMessage, AIMessage,
                        RemoveMessage, BaseModel patterns
3. langchain-chroma   â†’ Chroma class, similarity_search_with_score,
                        max_marginal_relevance_search, PersistentClient
4. chromadb           â†’ PersistentClient API, delete_collection
5. langchain-ollama   â†’ ChatOllama, OllamaEmbeddings, with_structured_output
6. langchain-text-splitters â†’ MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
7. pydantic           â†’ v2 BaseModel, Field patterns
8. gradio             â†’ v6.6.0 â€” ChatInterface, Blocks, gr.State, gr.JSON,
                        gr.Chatbot, streaming via yield, themes API
```

---

## PHASE 0 â€” Initialisation du projet

### `pyproject_agentic.toml`

```toml
[project]
name = "haproxy-agentic-rag"
version = "0.1.0"
description = "Agentic RAG sur documentation HAProxy 3.2 avec LangGraph + ChromaDB"
requires-python = ">=3.11"

dependencies = [
    # VÃ©rifier versions exactes via context7 avant de figer
    "langgraph>=0.2",
    "langchain-core>=0.3",
    "langchain-ollama>=0.2",       # LLM + embeddings (qwen3-embedding:8b via Ollama)
    "langchain-chroma>=0.1",
    "langchain-text-splitters>=0.3",
    "chromadb>=0.5",
    "gradio==6.6.0",           # version identique au projet principal
    "pydantic>=2.0",
    "httpx>=0.27",
    "beautifulsoup4>=4.12",
    "requests>=2.31",
    "pytest>=8.0",
    "pytest-asyncio>=0.23",
]
```

### `config_agentic.py`

```python
"""Configuration centralisÃ©e â€” modifier ici uniquement."""
from pathlib import Path

# Chemins
BASE_DIR = Path(__file__).parent
DATA_DIR = BASE_DIR / "data_agentic"
INDEX_DIR = BASE_DIR / "index_agentic"
CHROMA_PATH = str(INDEX_DIR / "chroma_db")
PARENT_STORE_PATH = BASE_DIR / "parent_store"

# HAProxy docs
HAPROXY_BASE_URL = "https://docs.haproxy.org/3.2/"
HAPROXY_DOC_SECTIONS = [
    "configuration.html",
    "management.html",
    "intro.html",
    "architecture.html",
]

# ChromaDB
COLLECTION_NAME = "haproxy_child_chunks"

# Chunking
CHILD_CHUNK_SIZE = 500
CHILD_CHUNK_OVERLAP = 80
MIN_PARENT_SIZE = 1500
MAX_PARENT_SIZE = 8000

# ChromaDB score threshold (distance cosine : plus petit = plus proche)
SCORE_THRESHOLD = 1.2   # Ã  calibrer via 06_eval_parent_child.py

# ModÃ¨les Ollama
LLM_MODEL = "qwen3:latest"
EMBEDDING_MODEL = "qwen3-embedding:8b"   # identique au projet principal

# Retrieval
DEFAULT_K_CHILD = 5
DEFAULT_K_MMR = 5
MMR_FETCH_K = 20

# CrÃ©ation des dossiers au premier import
for d in [DATA_DIR, INDEX_DIR, PARENT_STORE_PATH]:
    d.mkdir(parents=True, exist_ok=True)
```

---

## PHASE 1 â€” Scraping + vÃ©rification complÃ¨te

> â›” **RÃˆGLE DE LA PHASE 1 : NE PAS PASSER Ã€ LA PHASE 2 AVANT VALIDATION HUMAINE EXPLICITE.**
> L'agent s'arrÃªte aprÃ¨s chaque Ã©tape de cette phase, affiche un rapport lisible, et attend un `âœ… OK` de l'utilisateur. Si l'utilisateur signale un problÃ¨me, l'agent corrige et relance avant de continuer.

---

### Ã‰tape 1.0 â€” RÃ©fÃ©rence : compter les donnÃ©es du projet principal

Avant de scraper quoi que ce soit, l'agent doit **compter ce que le projet principal a dÃ©jÃ  scraÃ©** pour avoir une cible de comparaison.

```python
# Ã€ exÃ©cuter en premier â€” script de comptage de rÃ©fÃ©rence
# scraper/00_count_existing_data.py

import json
from pathlib import Path

# Chercher les donnÃ©es brutes du projet principal
# Typiquement dans data/ Ã  la racine du repo
candidates = [
    Path("data/scraped_pages.json"),
    Path("data/chunks.json"),
    Path("data/documents.json"),
    Path("data/"),       # lister le contenu si pas de nom connu
]

for path in candidates:
    if path.is_file():
        data = json.loads(path.read_text())
        print(f"ğŸ“„ {path} â†’ {len(data)} entrÃ©es")
        # Afficher un Ã©chantillon de 3 entrÃ©es pour comprendre la structure
        for entry in data[:3]:
            print(f"   ClÃ©s : {list(entry.keys()) if isinstance(entry, dict) else type(entry)}")
    elif path.is_dir():
        files = list(path.glob("**/*"))
        print(f"ğŸ“ {path}/ â†’ {len(files)} fichiers")
        for f in files[:10]:
            size = f.stat().st_size if f.is_file() else "-"
            print(f"   {f.name} ({size} bytes)")
```

**â†’ L'agent affiche le rÃ©sultat et demande Ã  l'utilisateur :**
```
ğŸ“Š RÃ‰FÃ‰RENCE PROJET PRINCIPAL :
   - data/scraped_pages.json : X entrÃ©es
   - Structure d'une entrÃ©e : {clÃ©s trouvÃ©es}

â“ Ces chiffres vous semblent corrects par rapport Ã  ce que vous connaissez du projet ?
   RÃ©pondez OK pour continuer, ou prÃ©cisez si des donnÃ©es manquent.
```

---

### Ã‰tape 1.1 â€” Scraping initial

#### `scraper/haproxy_scraper.py`

**Objectif** : reproduire exactement le pÃ©rimÃ¨tre de scraping du projet principal, en ajoutant la hiÃ©rarchie parent/child.

**Avant d'Ã©crire le scraper**, l'agent doit lire `01_scrape.py` du projet principal pour :
- Identifier quelles URLs sont scrapÃ©es (liste exacte des pages)
- Comprendre les sÃ©lecteurs HTML dÃ©jÃ  utilisÃ©s et qui fonctionnent
- RepÃ©rer les Ã©ventuelles exclusions ou transformations de contenu

```python
# Ã€ lire avant de coder : ../01_scrape.py (projet principal)
# Objectif : copier la liste des URLs cibles, rÃ©utiliser les sÃ©lecteurs CSS validÃ©s
```

**Structure de sortie attendue pour chaque document** :
```python
{
    "url": "https://docs.haproxy.org/3.2/configuration.html#4",
    "title": "4. Global Parameters",
    "content": "...",           # texte nettoyÃ© â€” mÃªme nettoyage que 01_scrape.py
    "parent_url": "https://docs.haproxy.org/3.2/configuration.html",
    "parent_title": "Configuration Manual",
    "depth": 2,                 # 1=page, 2=section h2, 3=sous-section h3
    "section_path": ["Configuration Manual", "Global Parameters"],
    "anchor": "4",
    "source_file": "configuration"
}
```

**Logique** :
1. Lire `../01_scrape.py` pour rÃ©cupÃ©rer la liste exacte des URLs et les sÃ©lecteurs
2. Scraper les mÃªmes pages + extraire la hiÃ©rarchie h1/h2/h3 supplÃ©mentaire
3. Nettoyer le contenu de maniÃ¨re identique au projet principal
4. Sauvegarder dans `data_agentic/scraped_pages.json`

#### `01_scrape_verified.py`

```python
"""
Ã‰tape 1 : Scraping HAProxy docs avec vÃ©rification pas-Ã -pas.

WORKFLOW :
  1. Lire 01_scrape.py du projet principal pour rÃ©cupÃ©rer URLs + sÃ©lecteurs
  2. Scraper les mÃªmes pages en ajoutant l'extraction de hiÃ©rarchie
  3. Sauvegarder scraped_pages.json
  4. Afficher un rapport dÃ©taillÃ© et ATTENDRE validation utilisateur
  5. Seulement si OK â†’ gÃ©nÃ©rer hierarchy_report.json et terminer
"""
import sys
import json
from scraper.haproxy_scraper import scrape_haproxy_docs
from scraper.html_structure_analyzer import analyze_hierarchy
from config_agentic import DATA_DIR

def print_scraping_report(pages: list, reference_count: int):
    """Affiche un rapport lisible pour validation humaine."""

    # Stats globales
    total = len(pages)
    coverage_pct = (total / reference_count * 100) if reference_count else 0

    # Distribution par profondeur
    by_depth = {}
    for p in pages:
        d = p.get("depth", 1)
        by_depth[d] = by_depth.get(d, 0) + 1

    # Pages avec contenu vide ou trop court
    empty = [p for p in pages if len(p.get("content", "")) < 50]
    short = [p for p in pages if 50 <= len(p.get("content", "")) < 200]

    # Pages sans section_path
    no_path = [p for p in pages if not p.get("section_path")]

    # Top sections scrapÃ©es
    sections = {}
    for p in pages:
        sp = p.get("section_path", ["?"])
        top = sp[0] if sp else "?"
        sections[top] = sections.get(top, 0) + 1

    print("\n" + "="*60)
    print("ğŸ“Š RAPPORT DE SCRAPING â€” VALIDATION REQUISE")
    print("="*60)
    print(f"\nğŸ“ˆ Volume :")
    print(f"   Pages scrapÃ©es     : {total}")
    print(f"   RÃ©fÃ©rence projet   : {reference_count}")
    print(f"   Couverture         : {coverage_pct:.1f}%")

    print(f"\nğŸŒ² HiÃ©rarchie (depth) :")
    for depth, count in sorted(by_depth.items()):
        label = {1: "pages racine", 2: "sections h2", 3: "sous-sections h3"}
        print(f"   depth={depth} ({label.get(depth,'?')}) : {count}")

    print(f"\nğŸ“š Top sections :")
    for section, count in sorted(sections.items(), key=lambda x: -x[1])[:10]:
        print(f"   {section:<40} {count} pages")

    print(f"\nâš ï¸  Anomalies dÃ©tectÃ©es :")
    print(f"   Contenu vide (<50 chars)   : {len(empty)}")
    print(f"   Contenu court (50-200c)    : {len(short)}")
    print(f"   Sans section_path          : {len(no_path)}")

    if empty:
        print(f"\n   URLs vides :")
        for p in empty[:5]:
            print(f"   âš ï¸  {p['url']}")

    print("\n" + "="*60)
    print("â“ VALIDATION REQUISE AVANT DE CONTINUER")
    print("="*60)
    print("""
VÃ©rifiez :
  1. Le nombre de pages correspond-il Ã  ce que vous attendez ?
  2. Les sections listÃ©es couvrent-elles bien toute la doc HAProxy ?
  3. Y a-t-il des anomalies Ã  corriger (pages vides, manquantes) ?

â†’ RÃ©pondez O pour continuer vers l'Ã©tape 1.2 (analyse hiÃ©rarchie)
â†’ RÃ©pondez N pour corriger le scraper et relancer
""")

def wait_for_human_validation(step_name: str) -> bool:
    """Attend une confirmation humaine. Retourne True si OK."""
    while True:
        try:
            answer = input(f"[{step_name}] Continuer ? (O/N) : ").strip().upper()
            if answer == "O":
                return True
            elif answer == "N":
                print("âŒ Validation refusÃ©e. Corriger le problÃ¨me puis relancer.")
                return False
            else:
                print("   RÃ©pondre O (oui) ou N (non)")
        except EOFError:
            # Mode non-interactif (CI, tests) : on passe automatiquement
            print("   [Mode non-interactif] Passage automatique")
            return True

def main():
    # --- Ã‰tape 1.0 : Compter la rÃ©fÃ©rence ---
    reference_count = 0
    ref_path = DATA_DIR.parent / "data" / "scraped_pages.json"
    if ref_path.exists():
        ref_data = json.loads(ref_path.read_text())
        reference_count = len(ref_data)
        print(f"ğŸ“Œ RÃ©fÃ©rence projet principal : {reference_count} entrÃ©es dans {ref_path}")
    else:
        # Chercher d'autres fichiers de donnÃ©es dans data/
        data_dir = DATA_DIR.parent / "data"
        if data_dir.exists():
            for f in data_dir.glob("*.json"):
                try:
                    d = json.loads(f.read_text())
                    if isinstance(d, list) and len(d) > 0:
                        print(f"ğŸ“Œ RÃ©fÃ©rence candidate : {f.name} â†’ {len(d)} entrÃ©es")
                        reference_count = max(reference_count, len(d))
                except Exception:
                    pass
        if reference_count == 0:
            print("âš ï¸  Aucune rÃ©fÃ©rence trouvÃ©e dans data/ â€” la couverture sera estimÃ©e sans base de comparaison")

    # --- Ã‰tape 1.1 : Scraping ---
    print("\nğŸ“¡ DÃ©marrage du scraping HAProxy docs 3.2...")
    pages = scrape_haproxy_docs()
    print(f"âœ“ {len(pages)} pages scrapÃ©es")

    # Sauvegarde brute immÃ©diate
    out_path = DATA_DIR / "scraped_pages.json"
    out_path.write_text(json.dumps(pages, ensure_ascii=False, indent=2))
    print(f"ğŸ’¾ SauvegardÃ© : {out_path}")

    # --- Rapport + validation humaine ---
    print_scraping_report(pages, reference_count)

    if not wait_for_human_validation("Scraping"):
        sys.exit(1)

    # --- Ã‰tape 1.2 : Analyse hiÃ©rarchie (seulement si scraping validÃ©) ---
    print("\nğŸ” Analyse de la hiÃ©rarchie parent/child...")
    report = analyze_hierarchy(pages)

    report_path = DATA_DIR / "hierarchy_report.json"
    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2))

    print(f"\nğŸ“Š Rapport hiÃ©rarchie :")
    print(f"   Parent coverage   : {report['parent_coverage']:.1%}")
    print(f"   Orphelins         : {report['orphan_children']}")
    print(f"   Avg children/parent : {report.get('avg_children_per_parent', 'N/A')}")

    # Alerte si couverture insuffisante
    if report["parent_coverage"] < 0.90:
        print(f"\nâš ï¸  ATTENTION : parent_coverage {report['parent_coverage']:.1%} < 90%")
        print("   La hiÃ©rarchie parent/child est insuffisante pour le RAG.")
        print("   Causes possibles : pages scrapÃ©es trop plates, TOC non parsÃ©e.")
        print("   â†’ Indiquer si vous souhaitez continuer quand mÃªme ou corriger.")

    if not wait_for_human_validation("HiÃ©rarchie parent/child"):
        sys.exit(1)

    print("\nâœ… PHASE 1 VALIDÃ‰E â€” prÃªt pour la Phase 2 (chunking)")
    print(f"   Fichiers produits :")
    print(f"   â†’ {out_path}")
    print(f"   â†’ {report_path}")

if __name__ == "__main__":
    main()
```

---

### Ã‰tape 1.2 â€” Analyse de la hiÃ©rarchie

#### `scraper/html_structure_analyzer.py`

```python
def analyze_hierarchy(scraped_pages: list) -> dict:
    """
    Analyse la hiÃ©rarchie des pages scrapÃ©es.
    Retourne un rapport JSON avec :
    - total_pages          : nombre total de documents
    - depth_distribution   : {"1": N, "2": N, "3": N}
    - orphan_children      : liste des URLs depth>1 sans parent_url valide
    - parent_coverage      : float â€” % de children dont le parent existe dans le dataset
    - avg_children_per_parent : float
    - sections_root        : liste des sections de profondeur 1 (titres principaux)
    - missing_vs_reference : si rÃ©fÃ©rence fournie, liste des sections absentes
    """
```

---

### Ã‰tape 1.3 â€” Comparaison croisÃ©e avec le projet principal

Un script dÃ©diÃ© compare le contenu scraÃ© par l'agentic_rag avec celui du projet principal, **section par section**.

#### `scraper/compare_with_reference.py`

```python
"""
Compare scraped_pages.json (agentic) avec les donnÃ©es du projet principal.
Produit un rapport de diff lisible pour validation humaine.

Usage : uv run python agentic_rag/scraper/compare_with_reference.py
"""
import json
from pathlib import Path
from config_agentic import DATA_DIR

def load_reference() -> list:
    """Cherche et charge les donnÃ©es du projet principal."""
    candidates = [
        DATA_DIR.parent / "data" / "scraped_pages.json",
        DATA_DIR.parent / "data" / "documents.json",
        DATA_DIR.parent / "data" / "chunks.json",
    ]
    for path in candidates:
        if path.exists():
            data = json.loads(path.read_text())
            if isinstance(data, list):
                print(f"âœ“ RÃ©fÃ©rence chargÃ©e : {path} ({len(data)} entrÃ©es)")
                return data
    return []

def extract_urls(data: list) -> set:
    """Extrait toutes les URLs ou identifiants uniques d'un dataset."""
    urls = set()
    for entry in data:
        if isinstance(entry, dict):
            url = entry.get("url") or entry.get("source") or entry.get("id", "")
            if url:
                urls.add(url.split("#")[0])  # normaliser : ignorer les ancres
    return urls

def extract_sections(data: list) -> set:
    """Extrait les titres de sections uniques."""
    sections = set()
    for entry in data:
        if isinstance(entry, dict):
            title = entry.get("title") or entry.get("section") or ""
            if title:
                sections.add(title.strip())
    return sections

def extract_content_volume(data: list) -> dict:
    """Calcule le volume de contenu total et par section."""
    total_chars = sum(len(e.get("content", "") or e.get("text", "")) for e in data if isinstance(e, dict))
    return {"total_chars": total_chars, "entries": len(data)}

def main():
    # Charger les deux datasets
    reference = load_reference()
    agentic_path = DATA_DIR / "scraped_pages.json"

    if not agentic_path.exists():
        print("âŒ scraped_pages.json introuvable â€” lancer 01_scrape_verified.py d'abord")
        return

    agentic = json.loads(agentic_path.read_text())

    # Comparaisons
    ref_urls = extract_urls(reference)
    agt_urls = extract_urls(agentic)
    ref_sections = extract_sections(reference)
    agt_sections = extract_sections(agentic)
    ref_volume = extract_content_volume(reference)
    agt_volume = extract_content_volume(agentic)

    # URLs dans rÃ©fÃ©rence mais pas dans agentic (donnÃ©es manquantes)
    missing_urls = ref_urls - agt_urls
    # URLs dans agentic mais pas dans rÃ©fÃ©rence (donnÃ©es supplÃ©mentaires â€” normal)
    extra_urls = agt_urls - ref_urls
    # Sections manquantes
    missing_sections = ref_sections - agt_sections

    print("\n" + "="*60)
    print("ğŸ“Š COMPARAISON AGENTIC vs PROJET PRINCIPAL")
    print("="*60)

    print(f"\nğŸ“ˆ Volume global :")
    print(f"   Projet principal  : {ref_volume['entries']} entrÃ©es / {ref_volume['total_chars']:,} chars")
    print(f"   Agentic RAG       : {agt_volume['entries']} entrÃ©es / {agt_volume['total_chars']:,} chars")
    coverage = agt_volume['total_chars'] / ref_volume['total_chars'] * 100 if ref_volume['total_chars'] else 0
    print(f"   Couverture contenu: {coverage:.1f}%")

    print(f"\nğŸ”— URLs :")
    print(f"   Communes          : {len(ref_urls & agt_urls)}")
    print(f"   Manquantes        : {len(missing_urls)}")
    print(f"   SupplÃ©mentaires   : {len(extra_urls)} (nouvelles sections hiÃ©rarchiques â€” normal)")

    if missing_urls:
        print(f"\n   âš ï¸  URLs manquantes ({len(missing_urls)}) :")
        for url in sorted(missing_urls)[:20]:
            print(f"      - {url}")
        if len(missing_urls) > 20:
            print(f"      ... et {len(missing_urls)-20} autres")

    print(f"\nğŸ“š Sections :")
    print(f"   Projet principal  : {len(ref_sections)} sections uniques")
    print(f"   Agentic RAG       : {len(agt_sections)} sections uniques")

    if missing_sections:
        print(f"\n   âš ï¸  Sections manquantes ({len(missing_sections)}) :")
        for s in sorted(missing_sections)[:20]:
            print(f"      - {s}")

    print("\n" + "="*60)
    if len(missing_urls) == 0 and len(missing_sections) == 0:
        print("âœ… COUVERTURE COMPLÃˆTE â€” aucune donnÃ©e manquante dÃ©tectÃ©e")
    elif coverage >= 95:
        print(f"âš ï¸  COUVERTURE QUASI-COMPLÃˆTE ({coverage:.1f}%) â€” vÃ©rifier les manques ci-dessus")
    else:
        print(f"âŒ COUVERTURE INSUFFISANTE ({coverage:.1f}%) â€” scraper Ã  corriger avant de continuer")
    print("="*60)

    # Sauvegarder le rapport pour rÃ©fÃ©rence
    diff_report = {
        "reference_entries": ref_volume["entries"],
        "agentic_entries": agt_volume["entries"],
        "content_coverage_pct": round(coverage, 2),
        "missing_urls": sorted(missing_urls),
        "missing_sections": sorted(missing_sections),
        "extra_urls_count": len(extra_urls),
    }
    out = DATA_DIR / "scraping_diff_report.json"
    out.write_text(json.dumps(diff_report, ensure_ascii=False, indent=2))
    print(f"\nğŸ’¾ Rapport sauvegardÃ© : {out}")

if __name__ == "__main__":
    main()
```

**â†’ L'agent exÃ©cute ce script et affiche le rÃ©sultat. Il attend `âœ… OK` de l'utilisateur avant de poursuivre.** Si des URLs ou sections manquent, l'agent corrige `haproxy_scraper.py` et relance.

---

### Ã‰tape 1.4 â€” Tests automatiques

#### `tests/test_scraper_alignment.py`

```python
import json
import pytest
from pathlib import Path
from config_agentic import DATA_DIR

@pytest.fixture
def scraped_pages():
    path = DATA_DIR / "scraped_pages.json"
    if not path.exists():
        pytest.skip("scraped_pages.json non disponible â€” lancer 01_scrape_verified.py")
    return json.loads(path.read_text())

@pytest.fixture
def hierarchy_report():
    path = DATA_DIR / "hierarchy_report.json"
    if not path.exists():
        pytest.skip("hierarchy_report.json non disponible")
    return json.loads(path.read_text())

@pytest.fixture
def diff_report():
    path = DATA_DIR / "scraping_diff_report.json"
    if not path.exists():
        pytest.skip("scraping_diff_report.json non disponible â€” lancer compare_with_reference.py")
    return json.loads(path.read_text())

def test_all_children_have_parent(scraped_pages):
    """Toute page depth > 1 doit avoir un parent_url non vide."""
    children = [p for p in scraped_pages if p.get("depth", 1) > 1]
    orphans = [p for p in children if not p.get("parent_url")]
    assert len(orphans) == 0, f"{len(orphans)} orphelins : {[o['url'] for o in orphans[:3]]}"

def test_parent_coverage_above_90(hierarchy_report):
    """Au moins 90% des enfants ont un parent valide."""
    coverage = hierarchy_report["parent_coverage"]
    assert coverage >= 0.90, f"parent_coverage = {coverage:.1%} < 90%"

def test_no_duplicate_urls(scraped_pages):
    """Aucune URL dupliquÃ©e dans les pages scrapÃ©es."""
    urls = [p["url"] for p in scraped_pages]
    dupes = len(urls) - len(set(urls))
    assert dupes == 0, f"{dupes} URLs dupliquÃ©es"

def test_content_not_empty(scraped_pages):
    """Toutes les pages ont du contenu substantiel."""
    empty = [p["url"] for p in scraped_pages if len(p.get("content", "")) < 50]
    assert len(empty) == 0, f"{len(empty)} pages avec contenu insuffisant : {empty[:3]}"

def test_section_path_present(scraped_pages):
    """Chaque page doit avoir un section_path non vide."""
    no_path = [p["url"] for p in scraped_pages if not p.get("section_path")]
    assert len(no_path) == 0, f"{len(no_path)} pages sans section_path : {no_path[:3]}"

def test_coverage_vs_reference(diff_report):
    """La couverture du contenu par rapport au projet principal doit Ãªtre >= 95%."""
    coverage = diff_report.get("content_coverage_pct", 0)
    assert coverage >= 95.0, (
        f"Couverture {coverage:.1f}% < 95% â€” "
        f"URLs manquantes : {diff_report.get('missing_urls', [])[:5]}"
    )

def test_no_missing_sections(diff_report):
    """Aucune section du projet principal ne doit Ãªtre absente."""
    missing = diff_report.get("missing_sections", [])
    assert len(missing) == 0, f"{len(missing)} sections manquantes : {missing[:10]}"
```

---

### RÃ©sumÃ© du flux de validation Phase 1

```
Agent : lancer 01_scrape_verified.py
  â†“
Agent affiche : rapport de volume + sections scrapÃ©es
  â†“
Utilisateur valide (O) ou demande correction (N)
  â†“ (si O)
Agent : lancer compare_with_reference.py
  â†“
Agent affiche : diff vs projet principal (URLs + sections manquantes)
  â†“
Utilisateur valide (O) ou signale des manques (N)
  â†“ (si N) Agent corrige haproxy_scraper.py â†’ retour au dÃ©but
  â†“ (si O)
Agent : lancer pytest tests/test_scraper_alignment.py
  â†“
Agent affiche : rÃ©sultat des 7 tests
  â†“
Utilisateur valide (O) â†’ PHASE 1 TERMINÃ‰E, passage Phase 2 autorisÃ©
```

> **L'agent ne lance JAMAIS `02_chunking_parent_child.py` sans que l'utilisateur ait explicitement validÃ© la Phase 1.**

---

## PHASE 2 â€” Chunking Parent/Child

### `02_chunking_parent_child.py`

**StratÃ©gie de chunking** :
- **Parents** = sections entiÃ¨res extraites du scraping (groupÃ©es par `section_path` jusqu'Ã  profondeur 2)
- **Children** = sous-blocs dÃ©coupÃ©s par `RecursiveCharacterTextSplitter`
- Chaque child porte `parent_id` dans ses mÃ©tadonnÃ©es

```python
"""
Ã‰tape 2 : Chunking parent/child alignÃ© sur la hiÃ©rarchie HAProxy
- Parent chunks : sections complÃ¨tes (depth 1-2)
- Child chunks : sous-blocs de 500 chars avec overlap 80
- Sauvegarde parents en JSON dans parent_store/
- Sauvegarde children en JSON dans data_agentic/chunks_child.json (vÃ©rification)
"""

import json
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from config_agentic import *

def build_parent_chunks(scraped_pages: list) -> list[tuple[str, Document]]:
    """
    Regroupe les pages scrapÃ©es en chunks parents.
    RÃ¨gles :
    - depth == 1 : parent autonome (page entiÃ¨re)
    - depth == 2 : parent = page h2 complÃ¨te
    - depth == 3 : rattachÃ© Ã  son parent depth-2
    Merge les chunks trop petits (< MIN_PARENT_SIZE)
    Split les chunks trop grands (> MAX_PARENT_SIZE)
    """

def build_child_chunks(parent_pairs: list) -> list[Document]:
    """
    DÃ©coupe chaque parent en children.
    Chaque child.metadata contient :
    - parent_id : str (clÃ© du fichier JSON dans parent_store/)
    - source : str (URL de la page)
    - section_path : list[str]
    - depth : int
    """
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHILD_CHUNK_SIZE,
        chunk_overlap=CHILD_CHUNK_OVERLAP,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    ...

def save_parents(parent_pairs: list):
    """Sauvegarde chaque parent en fichier JSON dans parent_store/."""
    for parent_id, doc in parent_pairs:
        filepath = PARENT_STORE_PATH / f"{parent_id}.json"
        filepath.write_text(json.dumps({
            "page_content": doc.page_content,
            "metadata": doc.metadata
        }, ensure_ascii=False, indent=2))

def main():
    pages = json.loads((DATA_DIR / "scraped_pages.json").read_text())

    print("ğŸ”¨ Construction des chunks parents...")
    parent_pairs = build_parent_chunks(pages)
    save_parents(parent_pairs)
    print(f"âœ“ {len(parent_pairs)} parents sauvegardÃ©s")

    print("âœ‚ï¸  Construction des chunks enfants...")
    child_chunks = build_child_chunks(parent_pairs)
    print(f"âœ“ {len(child_chunks)} children gÃ©nÃ©rÃ©s")

    # Dump de vÃ©rification
    dump = [{"content": c.page_content[:200], "metadata": c.metadata}
            for c in child_chunks]
    (DATA_DIR / "chunks_child.json").write_text(json.dumps(dump, ensure_ascii=False, indent=2))

    print(f"ğŸ“Š Stats :")
    print(f"  - Parents : {len(parent_pairs)}")
    print(f"  - Children : {len(child_chunks)}")
    print(f"  - Ratio moyen : {len(child_chunks)/len(parent_pairs):.1f} children/parent")

if __name__ == "__main__":
    main()
```

### `tests/test_chunking.py`

```python
import json
import pytest
from pathlib import Path
from config_agentic import DATA_DIR, PARENT_STORE_PATH, MIN_PARENT_SIZE, MAX_PARENT_SIZE

@pytest.fixture
def child_chunks():
    path = DATA_DIR / "chunks_child.json"
    if not path.exists():
        pytest.skip("chunks_child.json non disponible â€” lancer 02_chunking_parent_child.py")
    return json.loads(path.read_text())

def test_every_child_has_valid_parent_id(child_chunks):
    """Chaque child doit avoir un parent_id qui pointe vers un fichier JSON existant."""
    invalid = []
    for chunk in child_chunks:
        pid = chunk["metadata"].get("parent_id", "")
        if not pid:
            invalid.append("MISSING")
            continue
        json_path = PARENT_STORE_PATH / f"{pid}.json"
        if not json_path.exists():
            invalid.append(pid)
    assert len(invalid) == 0, f"{len(invalid)} parent_ids invalides : {invalid[:5]}"

def test_parent_size_in_range():
    """95%+ des parents sont entre MIN et MAX chars."""
    parents = list(PARENT_STORE_PATH.glob("*.json"))
    assert len(parents) > 0, "Aucun parent trouvÃ©"
    sizes = [len(json.loads(p.read_text())["page_content"]) for p in parents]
    in_range = sum(MIN_PARENT_SIZE <= s <= MAX_PARENT_SIZE for s in sizes)
    ratio = in_range / len(sizes)
    assert ratio >= 0.90, f"Seulement {ratio:.1%} des parents sont dans la plage [{MIN_PARENT_SIZE}, {MAX_PARENT_SIZE}]"

def test_child_size_in_range(child_chunks):
    """95%+ des children ont entre 100 et 800 chars."""
    sizes = [len(c["content"]) for c in child_chunks]
    in_range = sum(100 <= s <= 800 for s in sizes)
    ratio = in_range / len(sizes)
    assert ratio >= 0.90, f"Seulement {ratio:.1%} des children sont dans la plage [100, 800]"

def test_children_text_is_subset_of_parent(child_chunks):
    """Ã‰chantillon : 20 children doivent apparaÃ®tre dans leur parent."""
    import random
    sample = random.sample(child_chunks, min(20, len(child_chunks)))
    failures = []
    for chunk in sample:
        pid = chunk["metadata"]["parent_id"]
        parent_path = PARENT_STORE_PATH / f"{pid}.json"
        parent = json.loads(parent_path.read_text())
        snippet = chunk["content"][:80]
        if snippet not in parent["page_content"]:
            failures.append(f"Child snippet non trouvÃ© dans parent {pid}")
    assert len(failures) == 0, f"{len(failures)} incohÃ©rences parent/child : {failures[:3]}"

def test_all_children_have_required_metadata(child_chunks):
    """Chaque child doit avoir : parent_id, source, section_path."""
    required_keys = {"parent_id", "source", "section_path"}
    missing = [c for c in child_chunks if not required_keys.issubset(c["metadata"].keys())]
    assert len(missing) == 0, f"{len(missing)} children sans mÃ©tadonnÃ©es complÃ¨tes"
```

---

## PHASE 3 â€” Indexation ChromaDB

### `db/chroma_manager.py`

```python
"""Gestion de l'instance ChromaDB partagÃ©e."""
import chromadb
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings
from config_agentic import CHROMA_PATH, COLLECTION_NAME, EMBEDDING_MODEL

def get_embeddings() -> OllamaEmbeddings:
    # MÃªme modÃ¨le que le projet principal â€” assure la cohÃ©rence des espaces vectoriels
    return OllamaEmbeddings(model=EMBEDDING_MODEL)

def get_chroma_client() -> chromadb.PersistentClient:
    return chromadb.PersistentClient(path=CHROMA_PATH)

def get_vector_store(reset: bool = False) -> Chroma:
    client = get_chroma_client()
    if reset:
        try:
            client.delete_collection(COLLECTION_NAME)
            print(f"âœ“ Collection '{COLLECTION_NAME}' supprimÃ©e")
        except Exception:
            pass
    return Chroma(
        client=client,
        collection_name=COLLECTION_NAME,
        embedding_function=get_embeddings(),
    )
```

### `03_indexing_chroma.py`

```python
"""
Ã‰tape 3 : Indexation des child chunks dans ChromaDB
- Charge les children depuis data_agentic/chunks_child.json
- Reconstruit les Documents LangChain avec mÃ©tadonnÃ©es complÃ¨tes
- Indexe dans ChromaDB (dense search)
- VÃ©rifie l'indexation avec une requÃªte test
"""

import json
from langchain_core.documents import Document
from db.chroma_manager import get_vector_store
from config_agentic import DATA_DIR, DEFAULT_K_CHILD

def main():
    print("ğŸ“‚ Chargement des child chunks...")
    raw = json.loads((DATA_DIR / "chunks_child.json").read_text())

    # Reconstruction Documents LangChain complets (pas le dump tronquÃ©)
    # â†’ recharger depuis 02_chunking pour avoir le contenu complet
    # (chunks_child.json est un dump de vÃ©rification tronquÃ©)
    from agentic_rag.chunking import rebuild_full_children  # Ã  implÃ©menter dans 02
    child_docs = rebuild_full_children()

    print(f"ğŸ“Š {len(child_docs)} documents Ã  indexer")

    print("ğŸ”¨ Initialisation ChromaDB (reset=True)...")
    vector_store = get_vector_store(reset=True)

    print("ğŸ“¥ Indexation en cours (batch de 100)...")
    batch_size = 100
    for i in range(0, len(child_docs), batch_size):
        batch = child_docs[i:i+batch_size]
        vector_store.add_documents(batch)
        print(f"  âœ“ Batch {i//batch_size + 1}/{(len(child_docs)//batch_size)+1}")

    # VÃ©rification post-indexation
    print("\nğŸ” VÃ©rification post-indexation...")
    test_results = vector_store.similarity_search("haproxy frontend configuration", k=3)
    if len(test_results) == 0:
        print("âŒ ERREUR : aucun rÃ©sultat pour la requÃªte test")
        sys.exit(1)

    print(f"âœ“ {len(test_results)} rÃ©sultats pour la requÃªte test")
    for r in test_results:
        print(f"  - [{r.metadata.get('section_path', ['?'])[0]}] {r.page_content[:80]}...")

    print("\nâœ… Indexation terminÃ©e")

if __name__ == "__main__":
    main()
```

### `tests/test_retrieval.py`

```python
import pytest
from db.chroma_manager import get_vector_store
from config_agentic import PARENT_STORE_PATH, SCORE_THRESHOLD, DEFAULT_K_CHILD, MMR_FETCH_K

@pytest.fixture(scope="module")
def vector_store():
    return get_vector_store(reset=False)

def test_search_returns_results(vector_store):
    """Une requÃªte HAProxy typique doit retourner des rÃ©sultats."""
    results = vector_store.similarity_search("configure frontend timeout", k=DEFAULT_K_CHILD)
    assert len(results) > 0, "Aucun rÃ©sultat â€” index vide ?"
    assert all("parent_id" in r.metadata for r in results)

def test_search_with_score_threshold(vector_store):
    """Les rÃ©sultats doivent avoir un score de distance < SCORE_THRESHOLD."""
    results = vector_store.similarity_search_with_score(
        "haproxy backend server health check", k=DEFAULT_K_CHILD
    )
    assert len(results) > 0
    scores = [score for _, score in results]
    below = sum(s < SCORE_THRESHOLD for s in scores)
    # Au moins la moitiÃ© sous le seuil pour une requÃªte pertinente
    assert below >= len(scores) // 2, f"Scores trop Ã©levÃ©s : {scores}"

def test_parent_retrieval_from_child(vector_store):
    """AprÃ¨s search_child, retrieve_parent doit retourner le contexte complet."""
    import json
    results = vector_store.similarity_search("haproxy acl rules", k=3)
    parent_ids = list({r.metadata["parent_id"] for r in results})

    loaded = []
    for pid in parent_ids:
        path = PARENT_STORE_PATH / f"{pid}.json"
        assert path.exists(), f"parent_store/{pid}.json introuvable"
        doc = json.loads(path.read_text())
        loaded.append(doc)

    assert len(loaded) > 0

def test_mmr_search_diversity(vector_store):
    """MMR doit retourner des rÃ©sultats issus de sections diffÃ©rentes."""
    results = vector_store.max_marginal_relevance_search(
        "haproxy configuration", k=DEFAULT_K_CHILD, fetch_k=MMR_FETCH_K
    )
    assert len(results) >= 3
    parent_ids = {r.metadata["parent_id"] for r in results}
    assert len(parent_ids) >= 3, "MMR ne diversifie pas assez les sections"

def test_unrelated_query_low_score(vector_store):
    """Une requÃªte sans rapport avec HAProxy doit avoir des scores Ã©levÃ©s (loin)."""
    results = vector_store.similarity_search_with_score(
        "recette de cuisine pour faire une tarte aux pommes", k=3
    )
    if results:
        scores = [s for _, s in results]
        assert all(s > 0.8 for s in scores), f"Scores trop bas pour requÃªte hors-sujet : {scores}"
```

---

## PHASE 4 â€” Agent LangGraph

### `rag_agent/graph_state.py`

```python
"""DÃ©finition de l'Ã©tat du graphe LangGraph."""
# VÃ©rifier avec context7 que MessagesState est toujours l'import correct
from langgraph.graph import MessagesState
from pydantic import BaseModel, Field
from typing import Annotated
import operator

class State(MessagesState):
    """Ã‰tat Ã©tendu avec mÃ©moire de conversation."""
    questionIsClear: bool = False
    conversation_summary: str = ""
    sources_used: list[str] = []    # parent_ids utilisÃ©s dans la rÃ©ponse
```

### `rag_agent/schemas.py`

```python
"""SchÃ©mas Pydantic v2 pour les sorties structurÃ©es."""
from pydantic import BaseModel, Field
from typing import List

class QueryAnalysis(BaseModel):
    """Analyse et rÃ©Ã©criture de la requÃªte utilisateur."""
    is_clear: bool = Field(
        description="True si la question est claire et peut Ãªtre recherchÃ©e"
    )
    questions: List[str] = Field(
        description="Liste de questions rÃ©Ã©crites, autonomes et optimisÃ©es pour la recherche"
    )
    clarification_needed: str = Field(
        default="",
        description="Explication de pourquoi la question est floue (si is_clear=False)"
    )
```

### `rag_agent/prompts.py`

```python
"""System prompts spÃ©cialisÃ©s HAProxy."""

AGENT_SYSTEM_PROMPT = """
Tu es un expert de la documentation HAProxy 3.2. Tu DOIS utiliser les outils
disponibles pour rÃ©pondre Ã  toute question.

WORKFLOW OBLIGATOIRE (Ã  suivre pour CHAQUE question) :

1. Appeler `search_child_chunks` avec la query (k=5)
2. Examiner les chunks retournÃ©s â€” identifier les parent_ids pertinents
3. Appeler `retrieve_parent_chunks` avec ces parent_ids pour le contexte complet
4. Si le contexte est insuffisant : reformuler la query et rechercher UNE FOIS de plus
5. RÃ©pondre en utilisant UNIQUEMENT les informations trouvÃ©es dans les chunks

RÃˆGLES STRICTES :
- Ne jamais inventer de configuration HAProxy
- Toujours citer la section source (section_path des mÃ©tadonnÃ©es)
- Si une rÃ©ponse contient un bloc de config HAProxy : appeler `validate_haproxy_config`
- Si aucune information trouvÃ©e : dire clairement "Cette information n'est pas dans la documentation HAProxy 3.2 disponible"
- Donner des exemples de configuration quand c'est pertinent

FORMAT DE RÃ‰PONSE :
- RÃ©pondre en franÃ§ais si la question est en franÃ§ais
- Inclure des blocs de code pour les configurations
- Mentionner la section de la doc en fin de rÃ©ponse : *Source : [section_path]*
"""

QUERY_REWRITE_SYSTEM_PROMPT = """
RÃ©Ã©cris la requÃªte utilisateur pour la rendre optimale pour une recherche
dans la documentation HAProxy 3.2.

INSTRUCTIONS :
1. RÃ©soudre les rÃ©fÃ©rences pronominales ("it", "Ã§a", "le") grÃ¢ce au contexte de conversation
2. Ã‰clater les questions multiples en sous-questions distinctes (max 3)
3. Utiliser la terminologie HAProxy exacte (frontend, backend, listen, acl, server, etc.)
4. Supprimer le remplissage conversationnel
5. Marquer comme unclear : gibberish, insultes, questions sans objet clair
"""
```

### `rag_agent/tools.py`

```python
"""Outils de retrieval pour l'agent LangGraph."""
import json
import sys
from typing import List
from pathlib import Path
from langchain_core.tools import tool
from db.chroma_manager import get_vector_store
from config_agentic import PARENT_STORE_PATH, DEFAULT_K_CHILD, SCORE_THRESHOLD

# Instance partagÃ©e (lazy init)
_vector_store = None

def _get_vs():
    global _vector_store
    if _vector_store is None:
        _vector_store = get_vector_store(reset=False)
    return _vector_store

@tool
def search_child_chunks(query: str, k: int = DEFAULT_K_CHILD) -> List[dict]:
    """
    Recherche les K chunks enfants les plus pertinents pour une query.

    Args:
        query: La question ou requÃªte de recherche
        k: Nombre de rÃ©sultats Ã  retourner (dÃ©faut: 5)

    Returns:
        Liste de dicts avec content, parent_id, source, section_path, score
    """
    try:
        results = _get_vs().similarity_search_with_score(query, k=k)
        filtered = [
            {
                "content": doc.page_content,
                "parent_id": doc.metadata.get("parent_id", ""),
                "source": doc.metadata.get("source", ""),
                "section_path": doc.metadata.get("section_path", []),
                "score": float(score),
            }
            for doc, score in results
            if score < SCORE_THRESHOLD
        ]
        return filtered if filtered else []
    except Exception as e:
        return [{"error": str(e)}]

@tool
def retrieve_parent_chunks(parent_ids: List[str]) -> List[dict]:
    """
    RÃ©cupÃ¨re le contexte complet des chunks parents par leurs IDs.

    Args:
        parent_ids: Liste des parent_id Ã  rÃ©cupÃ©rer

    Returns:
        Liste de dicts avec content, parent_id, metadata
    """
    unique_ids = list(set(parent_ids))
    results = []
    for pid in unique_ids:
        path = PARENT_STORE_PATH / f"{pid}.json"
        if path.exists():
            try:
                doc = json.loads(path.read_text())
                results.append({
                    "content": doc["page_content"],
                    "parent_id": pid,
                    "metadata": doc["metadata"]
                })
            except Exception as e:
                results.append({"error": f"Erreur lecture {pid}: {e}"})
        else:
            results.append({"error": f"parent_id introuvable: {pid}"})
    return results

@tool
def validate_haproxy_config(config_block: str) -> dict:
    """
    Valide un bloc de configuration HAProxy en utilisant le validateur du repo existant.
    Appeler uniquement si la rÃ©ponse contient un exemple de configuration HAProxy.

    Args:
        config_block: Le bloc de configuration Ã  valider

    Returns:
        Dict avec is_valid (bool) et errors (list)
    """
    try:
        # Wrapper autour de haproxy_validator.py existant dans le repo racine
        sys.path.insert(0, str(Path(__file__).parent.parent.parent))
        from haproxy_validator import validate_config
        result = validate_config(config_block)
        return {"is_valid": result.get("valid", False), "errors": result.get("errors", [])}
    except ImportError:
        return {"is_valid": None, "errors": ["haproxy_validator non disponible"]}
    except Exception as e:
        return {"is_valid": None, "errors": [str(e)]}
```

### `rag_agent/nodes.py`

```python
"""
NÅ“uds du graphe LangGraph.
IMPORTANT : vÃ©rifier avec context7 les imports exacts de langgraph avant de coder.
"""
from typing import Literal
# VÃ©rifier imports via context7 :
from langgraph.graph import MessagesState
from langchain_core.messages import (
    HumanMessage, AIMessage, SystemMessage, RemoveMessage
)
from langchain_ollama import ChatOllama
from rag_agent.graph_state import State
from rag_agent.schemas import QueryAnalysis
from rag_agent.prompts import AGENT_SYSTEM_PROMPT, QUERY_REWRITE_SYSTEM_PROMPT
from rag_agent.tools import search_child_chunks, retrieve_parent_chunks, validate_haproxy_config
from config_agentic import LLM_MODEL

# LLM instances
llm = ChatOllama(model=LLM_MODEL, temperature=0)
llm_structured = ChatOllama(model=LLM_MODEL, temperature=0.1)

# LLM avec outils bindÃ©s
llm_with_tools = llm.bind_tools([
    search_child_chunks,
    retrieve_parent_chunks,
    validate_haproxy_config
])

def summarize_conversation(state: State) -> dict:
    """
    RÃ©sume l'historique de conversation pour maintenir le contexte
    sans surcharger le LLM. Actif seulement si > 4 messages.
    """
    if len(state["messages"]) < 4:
        return {"conversation_summary": ""}

    relevant_msgs = [
        msg for msg in state["messages"][:-1]
        if isinstance(msg, (HumanMessage, AIMessage))
        and not getattr(msg, "tool_calls", None)
    ]
    if not relevant_msgs:
        return {"conversation_summary": ""}

    prompt = f"""RÃ©sume en 1-2 phrases maximum les sujets clÃ©s de cette conversation
sur la configuration HAProxy. Ignore les malentendus.

{chr(10).join(f'{"Utilisateur" if isinstance(m, HumanMessage) else "Assistant"}: {m.content}'
              for m in relevant_msgs[-6:])}

RÃ©sumÃ© concis :"""

    response = llm.invoke([SystemMessage(content=prompt)])
    return {"conversation_summary": response.content}

def analyze_and_rewrite_query(state: State) -> dict:
    """
    Analyse la requÃªte utilisateur et la rÃ©Ã©crit pour la recherche.
    DÃ©tecte les questions floues et demande une clarification (human-in-the-loop).
    """
    last_message = state["messages"][-1]
    summary = state.get("conversation_summary", "")

    context_section = (
        f"Contexte conversation :\n{summary}"
        if summary.strip()
        else "Contexte : premiÃ¨re question de la session"
    )

    prompt = f"""{QUERY_REWRITE_SYSTEM_PROMPT}

Question utilisateur : "{last_message.content}"
{context_section}

Analyse et retourne le rÃ©sultat structurÃ©."""

    llm_with_output = llm_structured.with_structured_output(QueryAnalysis)
    response = llm_with_output.invoke([SystemMessage(content=prompt)])

    if response.is_clear:
        # Supprimer les anciens messages pour repartir sur la question rÃ©Ã©crite
        delete_msgs = [
            RemoveMessage(id=m.id)
            for m in state["messages"]
            if not isinstance(m, SystemMessage)
        ]
        rewritten = (
            "\n".join(f"{i+1}. {q}" for i, q in enumerate(response.questions))
            if len(response.questions) > 1
            else response.questions[0]
        )
        return {
            "questionIsClear": True,
            "messages": delete_msgs + [HumanMessage(content=rewritten)]
        }
    else:
        clarification = response.clarification_needed or (
            "Je n'ai pas bien compris votre question sur HAProxy. "
            "Pourriez-vous prÃ©ciser ce que vous cherchez ? "
            "(ex: configuration d'un frontend, rÃ¨gle ACL, paramÃ¨tre global...)"
        )
        return {
            "questionIsClear": False,
            "messages": [AIMessage(content=clarification)]
        }

def human_input_node(state: State) -> dict:
    """NÅ“ud placeholder pour l'interruption human-in-the-loop."""
    return {}

def agent_node(state: State) -> dict:
    """NÅ“ud principal de l'agent â€” utilise les outils pour rÃ©pondre."""
    messages = [SystemMessage(content=AGENT_SYSTEM_PROMPT)] + state["messages"]
    response = llm_with_tools.invoke(messages)
    return {"messages": [response]}
```

### `rag_agent/edges.py`

```python
"""Logique de routing conditionnel du graphe."""
from typing import Literal
from rag_agent.graph_state import State

def route_after_rewrite(state: State) -> Literal["agent", "human_input"]:
    """Route vers l'agent si la question est claire, sinon attend l'humain."""
    return "agent" if state.get("questionIsClear", False) else "human_input"
```

### `rag_agent/graph.py`

```python
"""
Construction et compilation du graphe LangGraph.
IMPORTANT : vÃ©rifier avec context7 les APIs StateGraph, ToolNode, tools_condition,
InMemorySaver, interrupt_before avant de coder.
"""
# VÃ©rifier imports via context7 :
from langgraph.graph import StateGraph, START
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.checkpoint.memory import InMemorySaver

from rag_agent.graph_state import State
from rag_agent.nodes import (
    summarize_conversation,
    analyze_and_rewrite_query,
    human_input_node,
    agent_node,
)
from rag_agent.edges import route_after_rewrite
from rag_agent.tools import search_child_chunks, retrieve_parent_chunks, validate_haproxy_config

def build_graph():
    """Construit et compile le graphe agentique."""
    checkpointer = InMemorySaver()
    builder = StateGraph(State)

    # NÅ“uds
    builder.add_node("summarize", summarize_conversation)
    builder.add_node("analyze_rewrite", analyze_and_rewrite_query)
    builder.add_node("human_input", human_input_node)
    builder.add_node("agent", agent_node)
    builder.add_node("tools", ToolNode([
        search_child_chunks,
        retrieve_parent_chunks,
        validate_haproxy_config
    ]))

    # Edges
    builder.add_edge(START, "summarize")
    builder.add_edge("summarize", "analyze_rewrite")
    builder.add_conditional_edges("analyze_rewrite", route_after_rewrite)
    builder.add_edge("human_input", "analyze_rewrite")
    builder.add_conditional_edges("agent", tools_condition)
    builder.add_edge("tools", "agent")

    # Compilation avec interruption avant human_input
    # VÃ‰RIFIER avec context7 : interrupt_before ou autre API
    return builder.compile(
        checkpointer=checkpointer,
        interrupt_before=["human_input"]
    )

# Instance partagÃ©e
agent_graph = build_graph()
```

### `tests/test_graph_flow.py`

```python
import pytest
from langchain_core.messages import HumanMessage
from rag_agent.graph import agent_graph

@pytest.fixture
def config():
    import uuid
    return {"configurable": {"thread_id": str(uuid.uuid4())}}

def test_graph_builds_without_error():
    """Le graphe doit se compiler sans exception."""
    from rag_agent.graph import build_graph
    g = build_graph()
    assert g is not None

def test_clear_haproxy_query_routes_to_agent(config):
    """Une question HAProxy claire doit atteindre l'agent."""
    result = agent_graph.invoke(
        {"messages": [HumanMessage("What is a frontend in HAProxy?")]},
        config
    )
    assert "messages" in result
    # Si interrupted â†’ questionIsClear est False, sinon il y a une vraie rÃ©ponse
    last = result["messages"][-1]
    assert last.content != ""

def test_unclear_query_triggers_clarification(config):
    """Une question floue doit dÃ©clencher une demande de clarification."""
    # Invoquer puis vÃ©rifier si interrupted ou si rÃ©ponse de clarification
    result = agent_graph.invoke(
        {"messages": [HumanMessage("blargh ???")]},
        config
    )
    assert not result.get("questionIsClear", True), "Query floue mais questionIsClear=True"

def test_conversation_memory_maintained(config):
    """La mÃ©moire de conversation doit permettre la rÃ©solution des pronoms."""
    # Tour 1
    agent_graph.invoke(
        {"messages": [HumanMessage("What is a frontend in HAProxy?")]},
        config
    )
    # Tour 2 â€” "it" doit Ãªtre rÃ©solu en "frontend"
    state = agent_graph.get_state(config)
    agent_graph.invoke(
        {"messages": [HumanMessage("How do I set a timeout for it?")]},
        config
    )
    final_state = agent_graph.get_state(config)
    # VÃ©rifier que la question rÃ©Ã©crite contient "frontend" ou "timeout"
    messages = final_state.values.get("messages", [])
    assert len(messages) > 0

def test_graph_completes_haproxy_question(config):
    """Une vraie question HAProxy doit produire une rÃ©ponse non vide."""
    result = agent_graph.invoke(
        {"messages": [HumanMessage("How to configure a basic HAProxy backend with health checks?")]},
        config
    )
    last = result["messages"][-1]
    assert len(last.content) > 100, "RÃ©ponse trop courte"

def test_response_cites_source(config):
    """Les rÃ©ponses doivent mentionner une source de la documentation."""
    result = agent_graph.invoke(
        {"messages": [HumanMessage("What is the maxconn parameter in HAProxy?")]},
        config
    )
    last = result["messages"][-1]
    # La rÃ©ponse doit contenir une rÃ©fÃ©rence Ã  la section
    assert "source" in last.content.lower() or "section" in last.content.lower() \
        or "configuration" in last.content.lower()
```

---

## PHASE 5 â€” Chatbot Gradio

### StratÃ©gie : copier et adapter `app/` existant

> **Principe** : ne pas rÃ©Ã©crire le chatbot Gradio from scratch. Copier le rÃ©pertoire `app/` du projet principal dans `agentic_rag/app/` et adapter **uniquement** les points de connexion au retriever. Tout le reste (UI, thÃ¨me, layout, gestion session) reste identique.

### Fichiers Ã  copier depuis `app/` â†’ `agentic_rag/app/`

```
app/                          â†’   agentic_rag/app/
â”œâ”€â”€ gradio_app.py             â†’   agentic_rag/app/gradio_app.py       â† MODIFIER (retriever)
â”œâ”€â”€ chat_interface.py         â†’   agentic_rag/app/chat_interface.py   â† MODIFIER (agent LangGraph)
â”œâ”€â”€ document_manager.py       â†’   agentic_rag/app/document_manager.py â† NE PAS MODIFIER
â””â”€â”€ rag_system.py             â†’   agentic_rag/app/rag_system.py       â† REMPLACER par agent
```

### Point d'entrÃ©e : `04_agentic_chatbot.py`

```python
"""
Chatbot Agentic RAG â€” copie de app/ adaptÃ©e pour pointer sur ChromaDB agentic.
Port : 7861 (le chatbot principal reste sur 7860).
"""
import sys
from pathlib import Path

# S'assurer que agentic_rag/ est dans le path
sys.path.insert(0, str(Path(__file__).parent))

from app.gradio_app import build_app

if __name__ == "__main__":
    demo = build_app()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7861,   # â† ne pas conflicuer avec 04_chatbot.py existant (7860)
        show_api=False,
    )
```

### Modifications dans `agentic_rag/app/rag_system.py`

C'est le **seul fichier Ã  rÃ©Ã©crire complÃ¨tement**. Il remplace le retriever_v3 par l'agent LangGraph. L'interface que `chat_interface.py` appelle reste identique.

```python
"""
rag_system.py â€” VERSION AGENTIQUE
Remplace retriever_v3 par l'agent LangGraph + ChromaDB.
L'interface publique (get_response, get_sources) reste identique Ã  l'original
pour ne pas modifier chat_interface.py.
"""
import uuid
from langchain_core.messages import HumanMessage

# Import de l'agent agentique (module parent agentic_rag/)
from rag_agent.graph import agent_graph

class AgenticRAGSystem:
    """
    Wrapper autour de agent_graph qui expose la mÃªme interface
    que le RAGSystem original de app/rag_system.py.
    Permet de brancher l'agent sans modifier chat_interface.py.
    """

    def __init__(self):
        self._sessions: dict[str, dict] = {}  # thread_id â†’ config LangGraph

    def create_session(self) -> str:
        """CrÃ©e une nouvelle session de conversation. Retourne le thread_id."""
        thread_id = str(uuid.uuid4())
        self._sessions[thread_id] = {
            "configurable": {"thread_id": thread_id}
        }
        return thread_id

    def get_response(self, message: str, thread_id: str) -> str:
        """
        Interface publique identique Ã  l'original.
        GÃ¨re automatiquement le human-in-the-loop :
        - Si l'agent est interrompu (question floue) â†’ retourne la demande de clarification
        - Si reprise aprÃ¨s clarification â†’ reprend le graph lÃ  oÃ¹ il Ã©tait
        """
        config = self._sessions.get(thread_id)
        if config is None:
            config = {"configurable": {"thread_id": thread_id}}
            self._sessions[thread_id] = config

        current_state = agent_graph.get_state(config)

        if current_state.next:
            # Graph interrompu en attente de clarification humaine
            agent_graph.update_state(
                config,
                {"messages": [HumanMessage(content=message.strip())]}
            )
            result = agent_graph.invoke(None, config)
        else:
            result = agent_graph.invoke(
                {"messages": [HumanMessage(content=message.strip())]},
                config
            )

        return result["messages"][-1].content

    def get_sources(self, thread_id: str) -> list[dict]:
        """
        Retourne les sources utilisÃ©es pour la derniÃ¨re rÃ©ponse.
        CompatibilitÃ© avec l'affichage sources du chatbot original.
        """
        config = self._sessions.get(thread_id)
        if not config:
            return []
        state = agent_graph.get_state(config)
        return state.values.get("sources_used", [])

    def reset_session(self, thread_id: str) -> str:
        """RÃ©initialise une session et retourne le nouveau thread_id."""
        if thread_id in self._sessions:
            del self._sessions[thread_id]
        return self.create_session()


# Instance partagÃ©e utilisÃ©e par chat_interface.py
rag_system = AgenticRAGSystem()
```

### Modifications dans `agentic_rag/app/chat_interface.py`

Changer **uniquement** la ligne d'import du rag_system :

```python
# AVANT (version originale)
# from app.rag_system import rag_system   â† pointe sur retriever_v3

# APRÃˆS (version agentique) â€” UNE SEULE LIGNE Ã€ CHANGER
from app.rag_system import rag_system     # â† pointe sur AgenticRAGSystem
```

> Tout le reste de `chat_interface.py` reste intact. L'interface `get_response(message, thread_id)` est identique.

### Modifications dans `agentic_rag/app/gradio_app.py`

Aucune modification de la logique UI. Changer uniquement le titre et le port dans `demo.launch()` :

```python
# Changer le titre pour distinguer les deux chatbots
gr.Markdown("# ğŸ”§ HAProxy Agentic RAG\n*LangGraph + ChromaDB | Parent/Child retrieval*")

# Changer le port (dÃ©jÃ  gÃ©rÃ© dans 04_agentic_chatbot.py, ne pas le dupliquer ici)
```

### Ce qui NE CHANGE PAS dans `app/`

- La structure des `gr.Blocks()`, `gr.Row()`, `gr.Column()`
- Le composant `gr.Chatbot` et ses paramÃ¨tres
- `gr.ChatInterface` et ses callbacks
- Le panel sources (`gr.JSON`)
- La gestion des sessions via `gr.State`
- Le thÃ¨me Gradio
- `document_manager.py` (non utilisÃ© dans la version agentique mais conservÃ© pour compatibilitÃ©)

### Checklist Gradio spÃ©cifique

```
[ ] Copier app/ â†’ agentic_rag/app/ (cp -r app/ agentic_rag/app/)
[ ] RÃ©Ã©crire agentic_rag/app/rag_system.py avec AgenticRAGSystem
[ ] Modifier la ligne d'import dans agentic_rag/app/chat_interface.py
[ ] Modifier le titre dans agentic_rag/app/gradio_app.py
[ ] 04_agentic_chatbot.py lance sur port 7861
[ ] Tester que le chatbot original (port 7860) fonctionne toujours
[ ] Tester que les deux chatbots peuvent tourner simultanÃ©ment
```

---

## PHASE 6 â€” Benchmarks & Ã‰valuation

### `05_bench_agentic.py`

**MÃ©triques comparÃ©es vs `retriever_v3` existant** :

```python
METRICS = {
    "answer_quality_score": "Score 0-1 Ã©valuÃ© par LLM judge",
    "retrieval_precision": "% chunks retournÃ©s pertinents (Ã©valuÃ© manuellement sur 10%)",
    "parent_child_utilization_rate": "% rÃ©ponses ayant utilisÃ© un chunk parent",
    "clarification_rate": "% questions ayant dÃ©clenchÃ© human-in-the-loop",
    "response_time_p50_sec": "MÃ©diane du temps de rÃ©ponse",
    "response_time_p95_sec": "95e percentile",
    "source_citation_rate": "% rÃ©ponses avec citation de section",
}

# Questions test issues de 05_bench_targeted.py existant (rÃ©utilisation directe)
# + nouvelles questions complexes multi-sections
```

**Output** : `data_agentic/bench_comparison.json` + rapport Markdown `data_agentic/BENCH_REPORT.md`

### `06_eval_parent_child.py`

```python
"""
Ã‰valuation de la valeur ajoutÃ©e de la stratÃ©gie parent/child.

Pour 50 questions :
  A) RÃ©ponse avec child chunks uniquement (contexte court)
  B) RÃ©ponse avec child + parent chunks (contexte Ã©tendu)

MÃ©triques :
  - QualitÃ© moyenne A vs B (LLM judge 0-1)
  - ComplÃ©tude : B contient-il plus d'informations que A ?
  - CohÃ©rence : les rÃ©ponses B sont-elles plus prÃ©cises ?

Output : data_agentic/parent_child_eval.json
Calibration du SCORE_THRESHOLD optimal
"""
```

---

## PHASE 7 â€” Export Dataset

### `07_export_dataset_agentic.py`

```python
"""
GÃ©nÃ©ration d'un dataset Q&A enrichi pour fine-tuning.

- Charge les questions depuis data/ existant
- GÃ©nÃ¨re des rÃ©ponses via l'agent agentique (contexte parent enrichi)
- Format JSONL compatible Ollama / OpenAI fine-tuning
- Inclut les mÃ©tadonnÃ©es sources (section_path, parent_id)

Structure de chaque entrÃ©e :
{
    "messages": [
        {"role": "system", "content": "Tu es un expert HAProxy 3.2..."},
        {"role": "user", "content": "Question..."},
        {"role": "assistant", "content": "RÃ©ponse avec contexte parent..."}
    ],
    "metadata": {
        "sources": ["section/path/1", "section/path/2"],
        "parent_ids": ["config_parent_42", "config_parent_17"],
        "quality_score": 0.85
    }
}

Output : data_agentic/dataset_agentic_qa.jsonl
"""
```

---

## PHASE 8 â€” Orchestrateur

### `00_rebuild_agentic.py`

```python
"""
Pipeline complet Agentic RAG â€” exÃ©cution sÃ©quentielle.

Usage :
  uv run python agentic_rag/00_rebuild_agentic.py
  uv run python agentic_rag/00_rebuild_agentic.py --skip-scrape
  uv run python agentic_rag/00_rebuild_agentic.py --skip-index
  uv run python agentic_rag/00_rebuild_agentic.py --test-only

Options :
  --skip-scrape  : utiliser les donnÃ©es scrapÃ©es existantes
  --skip-index   : utiliser l'index ChromaDB existant
  --test-only    : lancer uniquement les tests sans reconstruire
"""
import argparse
import subprocess
import sys
import time
import json
from datetime import datetime
from pathlib import Path
from config_agentic import DATA_DIR

STEPS = [
    ("01_scrape_verified.py",       "Scraping + analyse hiÃ©rarchie",     "~10-30 min"),
    ("02_chunking_parent_child.py", "Chunking parent/child",             "~2-5 min"),
    ("03_indexing_chroma.py",       "Indexation ChromaDB",               "~5-15 min"),
    ("06_eval_parent_child.py",     "Ã‰valuation couverture parent/child","~10-20 min"),
]

TEST_STEPS = [
    "tests/test_scraper_alignment.py",
    "tests/test_chunking.py",
    "tests/test_retrieval.py",
    "tests/test_graph_flow.py",
]

def run_step(script: str, label: str, eta: str) -> bool:
    print(f"\n{'='*60}")
    print(f"â–¶ {label} (ETA: {eta})")
    print(f"  Script : agentic_rag/{script}")
    print(f"{'='*60}")

    start = time.time()
    result = subprocess.run(
        [sys.executable, f"agentic_rag/{script}"],
        capture_output=False
    )
    elapsed = time.time() - start

    if result.returncode != 0:
        print(f"\nâŒ Ã‰CHEC : {label} ({elapsed:.1f}s)")
        return False

    print(f"\nâœ… OK : {label} ({elapsed:.1f}s)")
    return True

def run_tests() -> bool:
    print(f"\n{'='*60}")
    print("ğŸ§ª Lancement des tests pytest")
    print(f"{'='*60}")
    result = subprocess.run(
        [sys.executable, "-m", "pytest", "agentic_rag/tests/", "-v", "--tb=short"],
        capture_output=False
    )
    return result.returncode == 0

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--skip-scrape", action="store_true")
    parser.add_argument("--skip-index", action="store_true")
    parser.add_argument("--test-only", action="store_true")
    args = parser.parse_args()

    start_total = time.time()
    print(f"\nğŸš€ Pipeline Agentic RAG HAProxy")
    print(f"   DÃ©marrÃ© : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    report = {"steps": [], "tests": None, "total_time": None}

    if not args.test_only:
        for script, label, eta in STEPS:
            # Skip scrape et/ou index si demandÃ©
            if args.skip_scrape and "scrape" in script:
                print(f"â­ Skip : {label}")
                continue
            if args.skip_index and "indexing" in script:
                print(f"â­ Skip : {label}")
                continue

            ok = run_step(script, label, eta)
            report["steps"].append({"step": label, "success": ok})

            if not ok:
                print(f"\nğŸ’¥ Pipeline interrompu Ã  l'Ã©tape : {label}")
                sys.exit(1)

    # Tests
    tests_ok = run_tests()
    report["tests"] = {"success": tests_ok}

    total = time.time() - start_total
    report["total_time"] = f"{total:.1f}s"
    report["timestamp"] = datetime.now().isoformat()

    # Sauvegarde rapport
    (DATA_DIR / "pipeline_summary.json").write_text(
        json.dumps(report, ensure_ascii=False, indent=2)
    )

    print(f"\n{'='*60}")
    if tests_ok:
        print(f"âœ… PIPELINE COMPLET â€” {total:.0f}s")
        print(f"   Lancer le chatbot : uv run python agentic_rag/04_agentic_chatbot.py")
    else:
        print(f"âš ï¸  PIPELINE TERMINÃ‰ AVEC ERREURS DE TEST â€” {total:.0f}s")
        print(f"   Consulter : agentic_rag/data_agentic/pipeline_summary.json")
    print(f"{'='*60}\n")

if __name__ == "__main__":
    main()
```

---

## CHECKLIST FINALE POUR L'AGENT DE CODAGE

### Avant de commencer
```
[ ] context7 consultÃ© pour : langgraph, langchain-chroma, langchain-core,
    langchain-ollama, langchain-huggingface, chromadb, pydantic v2, gradio 6.6.0
[ ] Versions exactes figÃ©es dans pyproject_agentic.toml
[ ] Ollama lancÃ© localement avec qwen3:latest et qwen3-embedding:8b disponibles
    (ollama pull qwen3:latest && ollama pull qwen3-embedding:8b)
```

### Phase 1 â€” Scraping â›” VALIDATION HUMAINE OBLIGATOIRE
```
[ ] Ã‰tape 1.0 : lire 01_scrape.py existant â†’ identifier URLs cibles et sÃ©lecteurs CSS
[ ] Ã‰tape 1.0 : compter les donnÃ©es du projet principal dans data/ â†’ noter le chiffre de rÃ©fÃ©rence
[ ] Ã‰tape 1.1 : 01_scrape_verified.py lancÃ© â†’ scraped_pages.json produit
[ ] Ã‰tape 1.1 : rapport affichÃ© Ã  l'utilisateur (volume, sections, anomalies)
[ ] âœ… VALIDATION HUMAINE 1 : utilisateur confirme le volume et les sections
[ ] Ã‰tape 1.2 : compare_with_reference.py lancÃ© â†’ scraping_diff_report.json produit
[ ] Ã‰tape 1.2 : diff affichÃ© (URLs et sections manquantes vs projet principal)
[ ] âœ… VALIDATION HUMAINE 2 : utilisateur confirme couverture complÃ¨te (ou corrections faites)
[ ] Ã‰tape 1.3 : test_scraper_alignment.py : 7/7 tests PASS
[ ] âœ… VALIDATION HUMAINE 3 : utilisateur dit explicitement "passer Ã  la Phase 2"
[ ] hierarchy_report.json : parent_coverage >= 90%
[ ] scraping_diff_report.json : content_coverage_pct >= 95%
[ ] AUCUNE URL du projet principal manquante dans missing_urls
```

### Phase 2 â€” Chunking â›” NE PAS DÃ‰MARRER SANS VALIDATION PHASE 1
```
[ ] 02_chunking_parent_child.py : X parents + Y children gÃ©nÃ©rÃ©s (afficher les stats)
[ ] parent_store/ : N fichiers JSON prÃ©sents
[ ] chunks_child.json : dump de vÃ©rification prÃ©sent
[ ] test_chunking.py : 5/5 tests PASS
[ ] Ratio children/parent affichÃ© et cohÃ©rent (typiquement 8-15)
[ ] âœ… VALIDATION HUMAINE : utilisateur valide les stats avant Phase 3
```

### Phase 3 â€” Indexation â›” NE PAS DÃ‰MARRER SANS VALIDATION PHASE 2
```
[ ] 03_indexing_chroma.py : index construit sans erreur
[ ] RequÃªte test post-indexation : rÃ©sultats retournÃ©s
[ ] test_retrieval.py : 4/4 tests PASS
[ ] SCORE_THRESHOLD calibrÃ© (noter la valeur optimale dans config_agentic.py)
[ ] âœ… VALIDATION HUMAINE : utilisateur valide 3 exemples de recherche avant Phase 4
```

### Phase 4 â€” Agent
```
[ ] rag_agent/__init__.py prÃ©sent
[ ] graph.py compile sans erreur (agent_graph buildÃ©)
[ ] tools.py : 3 outils fonctionnels (search_child, retrieve_parent, validate_config)
[ ] validate_haproxy_config : wrap haproxy_validator.py existant sans l'importer de maniÃ¨re rigide
[ ] test_graph_flow.py : 5/5 tests PASS
```

### Phase 5 â€” Chatbot
```
[ ] app/ copiÃ© dans agentic_rag/app/ (cp -r)
[ ] agentic_rag/app/rag_system.py rÃ©Ã©crit avec AgenticRAGSystem
[ ] Import rag_system modifiÃ© dans agentic_rag/app/chat_interface.py (1 ligne)
[ ] Titre modifiÃ© dans agentic_rag/app/gradio_app.py
[ ] 04_agentic_chatbot.py dÃ©marre sur localhost:7861 sans erreur
[ ] Le chatbot original (port 7860) fonctionne toujours aprÃ¨s la copie
[ ] Les deux chatbots tournent simultanÃ©ment sans conflit
[ ] Human-in-the-loop : question floue â†’ demande de clarification affichÃ©e dans Gradio
[ ] MÃ©moire de conversation : "it" rÃ©solu correctement aprÃ¨s un premier Ã©change
```

### Phase 6 â€” Benchmarks
```
[ ] 05_bench_agentic.py : bench_comparison.json gÃ©nÃ©rÃ©
[ ] 06_eval_parent_child.py : parent_child_eval.json gÃ©nÃ©rÃ©
[ ] SCORE_THRESHOLD mis Ã  jour dans config_agentic.py selon rÃ©sultats
```

### Phase 7 â€” Dataset
```
[ ] 07_export_dataset_agentic.py : dataset_agentic_qa.jsonl gÃ©nÃ©rÃ©
[ ] Format JSONL valide (chaque ligne parseable par json.loads)
```

### Phase 8 â€” Pipeline complet
```
[ ] 00_rebuild_agentic.py : pipeline complet tourne sans intervention
[ ] data_agentic/pipeline_summary.json gÃ©nÃ©rÃ© avec succÃ¨s
[ ] --skip-scrape et --skip-index fonctionnels
[ ] README_AGENTIC.md : instructions claires pour installation + usage
```

### VÃ©rification finale â€” Non-rÃ©gression
```
[ ] AUCUN fichier du repo racine modifiÃ©
[ ] AUCUN fichier du repo racine supprimÃ©
[ ] Les scripts existants 00_rebuild_all.py â†’ 07_bench_config_correction.py fonctionnent toujours
[ ] Le chatbot existant 04_chatbot.py dÃ©marre toujours sur port 7860
```

---

## RÃ‰SUMÃ‰ DES COMMANDES

```bash
# Installation
cd haproxy-dataset-generator
uv sync --project agentic_rag/pyproject_agentic.toml

# Pipeline complet (premiÃ¨re fois)
uv run python agentic_rag/00_rebuild_agentic.py

# Reconstruire sans re-scraper
uv run python agentic_rag/00_rebuild_agentic.py --skip-scrape

# Tests uniquement
uv run python agentic_rag/00_rebuild_agentic.py --test-only

# Lancer le chatbot agentique
uv run python agentic_rag/04_agentic_chatbot.py
# â†’ http://localhost:7861

# Lancer le chatbot existant (inchangÃ©)
uv run python 04_chatbot.py
# â†’ http://localhost:7860
```

---

*Plan gÃ©nÃ©rÃ© pour agent de codage IA â€” Version ChromaDB + LangGraph + Gradio 6.6.0*
*Repo source : laurentvv/haproxy-dataset-generator*
*Architecture rÃ©fÃ©rence : GiovanniPasq/agentic-rag-for-dummies*
